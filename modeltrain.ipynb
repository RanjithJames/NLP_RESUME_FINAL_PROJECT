{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Replace 'yourfile.csv' with the path to your CSV file\n",
    "file_path = R\"/Users/RanjithJames/Downloads/archive/Resume/Resume.csv\"\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Resume_str</th>\n",
       "      <th>Resume_html</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16852973</td>\n",
       "      <td>HR ADMINISTRATOR/MARKETING ASSOCIATE\\...</td>\n",
       "      <td>&lt;div class=\"fontsize fontface vmargins hmargin...</td>\n",
       "      <td>HR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>22323967</td>\n",
       "      <td>HR SPECIALIST, US HR OPERATIONS      ...</td>\n",
       "      <td>&lt;div class=\"fontsize fontface vmargins hmargin...</td>\n",
       "      <td>HR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>33176873</td>\n",
       "      <td>HR DIRECTOR       Summary      Over 2...</td>\n",
       "      <td>&lt;div class=\"fontsize fontface vmargins hmargin...</td>\n",
       "      <td>HR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27018550</td>\n",
       "      <td>HR SPECIALIST       Summary    Dedica...</td>\n",
       "      <td>&lt;div class=\"fontsize fontface vmargins hmargin...</td>\n",
       "      <td>HR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17812897</td>\n",
       "      <td>HR MANAGER         Skill Highlights  ...</td>\n",
       "      <td>&lt;div class=\"fontsize fontface vmargins hmargin...</td>\n",
       "      <td>HR</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ID                                         Resume_str  \\\n",
       "0  16852973           HR ADMINISTRATOR/MARKETING ASSOCIATE\\...   \n",
       "1  22323967           HR SPECIALIST, US HR OPERATIONS      ...   \n",
       "2  33176873           HR DIRECTOR       Summary      Over 2...   \n",
       "3  27018550           HR SPECIALIST       Summary    Dedica...   \n",
       "4  17812897           HR MANAGER         Skill Highlights  ...   \n",
       "\n",
       "                                         Resume_html Category  \n",
       "0  <div class=\"fontsize fontface vmargins hmargin...       HR  \n",
       "1  <div class=\"fontsize fontface vmargins hmargin...       HR  \n",
       "2  <div class=\"fontsize fontface vmargins hmargin...       HR  \n",
       "3  <div class=\"fontsize fontface vmargins hmargin...       HR  \n",
       "4  <div class=\"fontsize fontface vmargins hmargin...       HR  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from io import StringIO\n",
    "\n",
    "# URL pointing to the JSON Lines file\n",
    "url = 'https://raw.githubusercontent.com/mystery2life/NLP-Project/main/temp_patterns.jsonl'\n",
    "\n",
    "# Fetching the data from the URL\n",
    "response = requests.get(url)\n",
    "response.raise_for_status()  # This will raise an exception if there's an error\n",
    "\n",
    "# Converting JSON Lines string to a pandas DataFrame\n",
    "SkillData = pd.read_json(StringIO(response.text), lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-lg==3.7.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.7.1/en_core_web_lg-3.7.1-py3-none-any.whl (587.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m587.7/587.7 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /Users/RanjithJames/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from en-core-web-lg==3.7.1) (3.7.5)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/RanjithJames/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.0.12)\n",
      "Requirement already satisfied: setuptools in /Users/RanjithJames/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (65.5.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /Users/RanjithJames/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.7.3)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /Users/RanjithJames/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.4.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/RanjithJames/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.4.0)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/RanjithJames/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.1.3)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/RanjithJames/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.0.10)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /Users/RanjithJames/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.12.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/RanjithJames/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.4.8)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /Users/RanjithJames/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (8.2.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/RanjithJames/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.32.3)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/RanjithJames/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.0.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/RanjithJames/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.0.10)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/RanjithJames/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.0.9)\n",
      "Requirement already satisfied: jinja2 in /Users/RanjithJames/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.1.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/RanjithJames/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (24.0)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/RanjithJames/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.0.5)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /Users/RanjithJames/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.26.4)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/RanjithJames/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (4.66.4)\n",
      "Requirement already satisfied: language-data>=1.2 in /Users/RanjithJames/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.2.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.4 in /Users/RanjithJames/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.18.4)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /Users/RanjithJames/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (4.12.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/RanjithJames/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.7.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/RanjithJames/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/RanjithJames/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/RanjithJames/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2024.6.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/RanjithJames/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.7)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/RanjithJames/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/RanjithJames/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.1.5)\n",
      "Requirement already satisfied: rich>=10.11.0 in /Users/RanjithJames/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (13.7.1)\n",
      "Requirement already satisfied: click>=8.0.0 in /Users/RanjithJames/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /Users/RanjithJames/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.5.4)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /Users/RanjithJames/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (7.0.4)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /Users/RanjithJames/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.18.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/RanjithJames/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.1.5)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in /Users/RanjithJames/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/RanjithJames/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (2.18.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/RanjithJames/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (3.0.0)\n",
      "Requirement already satisfied: wrapt in /Users/RanjithJames/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (1.16.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/RanjithJames/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-lg==3.7.1) (0.1.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_lg')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the pre-trained spaCy model\n",
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import Matcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Add patterns directly without using eval\n",
    "for index, row in SkillData.iterrows():\n",
    "    pattern = row['pattern']  # Assuming this is already a list of dictionaries\n",
    "    matcher.add(\"SKILL\", [pattern])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in /Users/RanjithJames/.pyenv/versions/3.10.14/lib/python3.10/site-packages (4.66.4)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def extract_skills(resume_texts):\n",
    "    training_data = []\n",
    "    for resume in tqdm(resume_texts, desc=\"Extracting skills\"):\n",
    "        doc = nlp(resume)\n",
    "        matches = matcher(doc)\n",
    "        spans = [doc[start:end] for _, start, end in matches]\n",
    "        entities = [(span.start_char, span.end_char, \"SKILL\") for span in spans if span.text.strip()]\n",
    "        training_data.append((resume, {\"entities\": entities}))\n",
    "    return training_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting skills: 100%|██████████| 2484/2484 [06:59<00:00,  5.91it/s]\n"
     ]
    }
   ],
   "source": [
    "# Limit the data to the first 5 resumes\n",
    "resume_texts_sample = data['Resume_str'].tolist()\n",
    "\n",
    "# Extract skills from the limited sample\n",
    "training_data_sample = extract_skills(resume_texts_sample)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save sample_training_data to a file\n",
    "# with open('sample_training_data.pkl', 'wb') as f:\n",
    "#     pickle.dump(sample_training_data, f)\n",
    "\n",
    "# Load sample_training_data from the file\n",
    "with open('sample_training_data.pkl', 'rb') as f:\n",
    "    training_data_sample = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"         HR SPECIALIST, US HR OPERATIONS       Summary     Versatile  media professional with background in Communications, Marketing, Human Resources and Technology.\\xa0        Experience     09/2015   to   Current     HR Specialist, US HR Operations    Company Name   －   City  ,   State       Managed communication regarding launch of Operations group, policy changes and system outages      Designed standard work and job aids to create comprehensive training program for new employees and contractors         Audited job postings for old, pending, on-hold and draft positions.           Audited union hourly, non-union hourly and salary background checks and drug screens             Conducted monthly new hire benefits briefing to new employees across all business units               Served as a link between HR Managers and vendors by handling questions and resolving system-related issues         Provide real-time process improvement feedback on key metrics and initiatives  Successfully re-branded US HR Operations SharePoint site  Business Unit project manager for RFI/RFP on Background Check and Drug Screen vendor         01/2014   to   05/2015     IT, Marketing and Communications Co-op    Company Name   －   City  ,   State      Posted new articles, changes and updates to corporate SharePoint site including graphics and visual communications.  Researched and drafted articles and feature stories to promote company activities and programs.  Co-edited and developed content for quarterly published newsletter.  Provided communication support for internal and external events.  Collaborated with Communication team, media professionals and vendors to determine program needs for print materials, web design and digital communications.  Entrusted to lead product, service and software launches for Digital Asset Management tool, Marketing Toolkit website and Executive Tradeshows Calendar.  Created presentations for management and executive approval to ensure alignment with corporate guidelines and branding.  Maintained the MySikorsky SharePoint site and provided timely solutions to mitigate issues.\\xa0\\xa0\\xa0\\xa0  Created story board and produced video for annual IT All Hands meeting.         10/2012   to   01/2014     Relationship Coordinator/Marketing Specialist    Company Name   －   City  ,   State       Partnered with vendor to manage the in-house advertising program consisting of print and media collateral pieces.     Coordinated pre-show and post-show activities at trade shows.     Managed marketing campaigns to generate new business and to support partner and sales teams.     Ordered marketing collateral for meetings, trade shows and advisors.    Improved, administered and modified marketing programs to increase product awareness.  Assisted in preparing internal promotional publications, managed marketing material inventory and supervised distribution of publications to ensure high quality product output.  Coordinated marketing materials including brochures, promotional materials and products.  Partnered with graphic designers to develop appropriate materials and branding for brochures.  Used tracking and reporting systems for sales leads and appointments.         09/2009   to   10/2012     Assistant Head Teller    Company Name   －   City  ,   State       Received an internal audit score of  100 %.     Performed daily and monthly audits of ATM machines and tellers.     Educated customers on a variety of retail products and available credit options.       Consistently met or exceeded quarterly sales goals     Promoted products and services to\\ncustomers while maintaining company brand identity\\n\\n·\\xa0\\xa0\\xa0\\xa0\\n  Implemented programs to achieve\\nand exceed customer and company participation goals\\xa0\\n\\n\\xa0  Organized company sponsored events on campus resulting in increased\\nbrand awareness\\n\\n·\\xa0\\xa0\\xa0\\xa0\\n  Coached peers on\\nthe proper use of programs to improve work flow efficiency  Utilized product knowledge to successfully sell\\nto and refer clients based on individual needs  Promoted marketing the grand opening\\nof new branch locations to strengthen company brand affinity\\n\\n·\\xa0\\xa0\\xa0\\xa0   Organized company sponsored events\\nresulting in increased brand awareness and improved sales\\n\\n·\\xa0\\xa0\\xa0\\xa0   Coached peers on the proper use of\\nprograms to increase work flow efficiency\\n\\n          Senior Producer - 2014 SHU Media Exchange    Company Name   －   City  ,   State      Planned and executed event\\xa0focusing on Connecticut's creative corridor, growth of industry and opportunities that come with development. A\\xa0 panel of industry professionals addressed topics related to media and hosted a question and answer session for approximately 110 attendees. Following the forum, guests were invited to engage in networking and conversation at a post-event reception.         Education     2014     Master of Arts  :   Corporate Communication & Public Relations    Sacred Heart University   －   City  ,   State             2013     Bachelor of Arts  :   Relational Communication    Western Connecticut State University   －   City  ,   State              Skills    Adobe Photoshop, ADP, Asset Management, branding, brochures, content, Customer Care, Final Cut Pro, graphics, graphic, HR, Illustrator, InDesign, Innovation, inventory, Lotus Notes, marketing, marketing materials, marketing material, materials, Microsoft Office, SharePoint, newsletter, presentations, process improvement, Project Management, promotional materials, publications, Quality, real-time, Recruitment, reporting, RFP, sales, stories, Employee Development, video, web design, website, articles   \", {'entities': [(108, 122, 'SKILL'), (124, 133, 'SKILL'), (759, 767, 'SKILL'), (1040, 1048, 'SKILL'), (1164, 1173, 'SKILL'), (1178, 1192, 'SKILL'), (1342, 1356, 'SKILL'), (1548, 1555, 'SKILL'), (1713, 1719, 'SKILL'), (1732, 1746, 'SKILL'), (1788, 1796, 'SKILL'), (1841, 1850, 'SKILL'), (2253, 2262, 'SKILL'), (2363, 2374, 'SKILL'), (2510, 2519, 'SKILL'), (2546, 2554, 'SKILL'), (2562, 2569, 'SKILL'), (2607, 2616, 'SKILL'), (2707, 2716, 'SKILL'), (2823, 2832, 'SKILL'), (2833, 2841, 'SKILL'), (2948, 2957, 'SKILL'), (4008, 4017, 'SKILL'), (5066, 5081, 'SKILL'), (5248, 5257, 'SKILL'), (5259, 5268, 'SKILL'), (5280, 5289, 'SKILL'), (5290, 5298, 'SKILL'), (5389, 5407, 'SKILL'), (5544, 5550, 'SKILL')]})\n"
     ]
    }
   ],
   "source": [
    "print(training_data_sample[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.5.1-cp310-cp310-macosx_12_0_arm64.whl (11.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.0/11.0 MB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.19.5 in /Users/RanjithJames/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /Users/RanjithJames/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/RanjithJames/.pyenv/versions/3.10.14/lib/python3.10/site-packages (from scikit-learn) (1.4.2)\n",
      "Collecting threadpoolctl>=3.1.0\n",
      "  Downloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scikit-learn\n",
      "Successfully installed scikit-learn-1.5.1 threadpoolctl-3.5.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming 'training_data_sample' is a list of tuples in the format (text, annotations)\n",
    "texts, annotations = zip(*training_data_sample)  # Unzip the training data\n",
    "\n",
    "# Split the data into train and validation sets\n",
    "train_texts, val_texts, train_annotations, val_annotations = train_test_split(\n",
    "    texts, annotations, test_size=0.2, random_state=42  # 20% of the data is used for validation\n",
    ")\n",
    "\n",
    "# Recreate the training and validation sets as tuples (text, annotation)\n",
    "train_data = list(zip(train_texts, train_annotations))\n",
    "val_data = list(zip(val_texts, val_annotations))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from spacy.training import Example\n",
    "\n",
    "\n",
    "\n",
    "# Get the NER pipeline component\n",
    "ner = nlp.get_pipe(\"ner\")\n",
    "\n",
    "# Function to remove overlapping entities\n",
    "def remove_overlapping_entities(entities):\n",
    "    entities = sorted(entities, key=lambda x: x[0])  # Sort by start position\n",
    "    non_overlapping_entities = []\n",
    "    last_end = -1\n",
    "    for start, end, label in entities:\n",
    "        if start >= last_end:  # No overlap\n",
    "            non_overlapping_entities.append((start, end, label))\n",
    "            last_end = end\n",
    "    return non_overlapping_entities\n",
    "\n",
    "# Add the 'SKILL' entity label to the NER model if it's not already known\n",
    "if 'SKILL' not in ner.labels:\n",
    "    ner.add_label(\"SKILL\")\n",
    "\n",
    "# Additional custom entities from training data\n",
    "for _, annotations in train_data:\n",
    "    for ent in annotations.get(\"entities\"):\n",
    "        if ent[2] not in ner.labels:\n",
    "            ner.add_label(ent[2])\n",
    "\n",
    "# Disable other pipeline components during training to train only NER\n",
    "unaffected_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration 0: 100%|██████████| 1987/1987 [05:59<00:00,  5.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Losses: {'ner': 14132.176602870706}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration 1: 100%|██████████| 1987/1987 [05:55<00:00,  5.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, Losses: {'ner': 4599.948075919623}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration 2: 100%|██████████| 1987/1987 [05:45<00:00,  5.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2, Losses: {'ner': 3663.249017594434}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration 3: 100%|██████████| 1987/1987 [05:45<00:00,  5.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3, Losses: {'ner': 3024.6585257083775}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration 4: 100%|██████████| 1987/1987 [05:45<00:00,  5.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4, Losses: {'ner': 2618.5894871750106}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration 5: 100%|██████████| 1987/1987 [05:46<00:00,  5.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5, Losses: {'ner': 2344.202397305668}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration 6: 100%|██████████| 1987/1987 [05:45<00:00,  5.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6, Losses: {'ner': 2072.3443961159433}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration 7: 100%|██████████| 1987/1987 [05:45<00:00,  5.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7, Losses: {'ner': 1998.7912388464115}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration 8: 100%|██████████| 1987/1987 [05:45<00:00,  5.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 8, Losses: {'ner': 1712.8712193033598}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration 9: 100%|██████████| 1987/1987 [05:44<00:00,  5.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 9, Losses: {'ner': 1584.0195122833247}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration 10: 100%|██████████| 1987/1987 [05:44<00:00,  5.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10, Losses: {'ner': 1433.7420755039723}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration 11: 100%|██████████| 1987/1987 [05:44<00:00,  5.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 11, Losses: {'ner': 1295.5600148454776}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration 12: 100%|██████████| 1987/1987 [05:45<00:00,  5.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 12, Losses: {'ner': 1243.8174947697212}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration 13: 100%|██████████| 1987/1987 [05:44<00:00,  5.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 13, Losses: {'ner': 1153.1047805332462}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration 14: 100%|██████████| 1987/1987 [05:45<00:00,  5.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 14, Losses: {'ner': 1098.044086336342}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration 15: 100%|██████████| 1987/1987 [05:44<00:00,  5.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 15, Losses: {'ner': 938.4581617694281}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration 16: 100%|██████████| 1987/1987 [05:44<00:00,  5.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 16, Losses: {'ner': 875.4256567449193}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration 17: 100%|██████████| 1987/1987 [05:46<00:00,  5.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 17, Losses: {'ner': 776.1399532494521}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration 18: 100%|██████████| 1987/1987 [05:46<00:00,  5.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 18, Losses: {'ner': 782.4025988516812}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration 19: 100%|██████████| 1987/1987 [05:46<00:00,  5.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 19, Losses: {'ner': 814.6872175465055}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "with nlp.disable_pipes(*unaffected_pipes):  # only train NER\n",
    "    optimizer = nlp.resume_training()\n",
    "    for itn in range(20):  # Number of training iterations\n",
    "        random.shuffle(train_data)\n",
    "        losses = {}\n",
    "        for text, annotations in tqdm(train_data, desc=f\"Iteration {itn}\"):\n",
    "            entities = remove_overlapping_entities(annotations.get(\"entities\"))\n",
    "            annotations['entities'] = entities  # Update annotations with non-overlapping entities\n",
    "            doc = nlp.make_doc(text)\n",
    "            example = Example.from_dict(doc, {'entities': entities})\n",
    "            nlp.update([example], drop=0.2, sgd=optimizer, losses=losses)\n",
    "        print(f\"Iteration {itn}, Losses: {losses}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Define a valid path for saving the model\n",
    "model_dir = R\"/Users/RanjithJames/Downloads/NLP-Project-project/data/model_new\"\n",
    "\n",
    "# Create the directory if it does not exist\n",
    "if not os.path.exists(model_dir):\n",
    "    os.makedirs(model_dir)\n",
    "\n",
    "# Save the trained model to disk\n",
    "nlp.to_disk(model_dir)\n",
    "\n",
    "# Load the model from the saved directory into a new variable\n",
    "newnlp = spacy.load(model_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokens import Span\n",
    "from spacy.training import Example\n",
    "from spacy.scorer import Scorer\n",
    "\n",
    "def evaluate(ner_model, examples):\n",
    "    scorer = Scorer()\n",
    "    examples_to_score = []\n",
    "    \n",
    "    for input_text, annot in examples:\n",
    "        # Create doc for gold text\n",
    "        doc_gold_text = ner_model.make_doc(input_text)\n",
    "        spans = [Span(doc_gold_text, start, end, label) for start, end, label in annot['entities'] if start >= 0 and end <= len(doc_gold_text)]\n",
    "        \n",
    "        # Set gold spans to the .ents attribute directly for correct scoring\n",
    "        doc_gold_text.ents = spans\n",
    "\n",
    "        # Process the text with the NER model to get predictions\n",
    "        pred_doc = ner_model(input_text)\n",
    "        \n",
    "        # Create an Example object with the predicted and gold docs\n",
    "        example = Example(pred_doc, doc_gold_text)\n",
    "        examples_to_score.append(example)\n",
    "\n",
    "    # Score the examples and calculate NER metrics\n",
    "    scores = scorer.score(examples_to_score)\n",
    "\n",
    "    # Return only NER-related metrics\n",
    "    return {\n",
    "        'ents_p': scores['ents_p'],\n",
    "        'ents_r': scores['ents_r'],\n",
    "        'ents_f': scores['ents_f'],\n",
    "        'ents_per_type': scores['ents_per_type']\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = \"Abid Ali Awan Data Scientist I am a certified data scientist professional, who loves building machine learning models and blogs about the latest AI technologies. I am currently testing AI Products at PEC-PITC, which later gets approved for human trials. abidaliawan@tutamail.com +923456855126 Islamabad, Pakistan abidaliawan.me WORK EXPERIENCE Data Scientist Pakistan Innovation and Testing Center - PEC 04/2021 - Present, Islamabad, Pakistan Redesigned data of engineers that were mostly scattered and unavailable. Designed dashboard and data analysis report to help higher management make better decisions. Accessibility of key information has created a new culture of making data-driven decisions. Contact: Ali Raza Asif - darkslayerraza10@gmail.com Data Scientist Freelancing/Kaggle 11/2020 - Present, Islamabad, Pakistan Engineered a healthcare system. Used machine learning to detect some of the common decisions. The project has paved the way for others to use new techniques to get better results. Participated in Kaggle machine learning competitions. Learned new techniques to get a better score and finally got to 1 percent rank. Researcher / Event Organizer CREDIT 02/2017 - 07/2017, Kuala Lumpur, Malaysia Marketing for newly build research lab. Organized technical events and successfully invited the multiple company's CEO for talks. Reduced the gap between industries and educational institutes. Research on new development in the IoT sector. Created research proposal for funding. Investigated the new communication protocol for IoT devices. Contact: Dr. Tan Chye Cheah - dr.chyecheah.t@apu.edu.my EDUCATION MSc in Technology Management Staffordshire University 11/2015 - 04/2017, Postgraduate with Distinction Challenges in Implementing IoT-enabled Smart cities in Malaysia. Bachelors Electrical Telecommunication Engineering COMSATS Institute of Information Technology, Islamabad 08/2010 - 01/2014, CGPA: 3.09 Networking Satellite communications Programming/ Matlab Telecommunication Engineering SKILLS Designing Leadership Media/Marketing R/Python SQL Tableau NLP Data Analysis Machine learning Deep learning Webapp/Cloud Feature Engineering Ensembling Time Series Technology Management ACHIEVEMENTS 98th Hungry Geese Simulation Competition (08/2021) 2nd in Covid-19 vaccinations around the world (07/2021) 8th in Automatic Speech Recognition in WOLOF (06/2021) Top 10 in WiDS Datathon. (03/2021) 40th / 622 in MagNet: Model the Geomagnetic Field Hosted by NOAA (02/2021) 18th in Rock, Paper, Scissors/Designing AI Agent Competition. (02/2021) PROJECTS Goodreads Profile Analysis WebApp (09/2021) Data Analysis Web Scraping XLM Interactive Visualization Contributed in orchest.io (08/2021) Testing and Debuging Technical Article Proposing new was to Improve ML pipelines World Vaccine Update System (06/2021) Used sqlite3 for database Automated system for daily update the Kaggle DB and Analysis Interactive dashboard mRNA-Vaccine-Degradation-Prediction (06/2021) Explore our dataset and then preprocessed sequence, structure, and predicted loop type features Train deep learning GRU model Trip Advisor Data Analysis/ML (04/2021) Preprocessing Data, Exploratory Data analysis, Word clouds. Feature Engineering, Text processing. BiLSTM Model for predicting rating, evaluation, model performance. Jane Street Market Prediction (03/2021) EDA, Feature Engineering, experimenting with hyperparameters. Ensembling: Resnet, NN Embeddings, TF Simple NN model. Using simple MLP pytorch model. Achievements/Tasks Achievements/Tasks Achievements/Tasks Thesis Courses\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI SKILL\n",
      "testing SKILL\n",
      "AI SKILL\n",
      "Testing SKILL\n",
      "data analysis SKILL\n",
      "Marketing SKILL\n",
      "Engineering SKILL\n",
      "communications SKILL\n",
      "Engineering SKILL\n",
      "Marketing SKILL\n",
      "R SKILL\n",
      "Python SKILL\n",
      "SQL SKILL\n",
      "Tableau SKILL\n",
      "Data Analysis SKILL\n",
      "Machine learning SKILL\n",
      "Engineering SKILL\n",
      "Simulation SKILL\n",
      "AI SKILL\n",
      "Data Analysis SKILL\n",
      "Visualization SKILL\n",
      "Testing SKILL\n",
      "ML SKILL\n",
      "database SKILL\n",
      "Data Analysis SKILL\n",
      "Data analysis SKILL\n",
      "Engineering SKILL\n",
      "Engineering SKILL\n"
     ]
    }
   ],
   "source": [
    "# Assuming 'newnlp' is your spaCy NER model already loaded\n",
    "doc = newnlp(test)  # Process the text through the model\n",
    "\n",
    "# Print the recognized entities and their labels\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
