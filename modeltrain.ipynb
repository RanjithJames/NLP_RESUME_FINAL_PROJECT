{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Resume_str</th>\n",
       "      <th>Resume_html</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16852973</td>\n",
       "      <td>HR ADMINISTRATOR/MARKETING ASSOCIATE\\...</td>\n",
       "      <td>&lt;div class=\"fontsize fontface vmargins hmargin...</td>\n",
       "      <td>HR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>22323967</td>\n",
       "      <td>HR SPECIALIST, US HR OPERATIONS      ...</td>\n",
       "      <td>&lt;div class=\"fontsize fontface vmargins hmargin...</td>\n",
       "      <td>HR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>33176873</td>\n",
       "      <td>HR DIRECTOR       Summary      Over 2...</td>\n",
       "      <td>&lt;div class=\"fontsize fontface vmargins hmargin...</td>\n",
       "      <td>HR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27018550</td>\n",
       "      <td>HR SPECIALIST       Summary    Dedica...</td>\n",
       "      <td>&lt;div class=\"fontsize fontface vmargins hmargin...</td>\n",
       "      <td>HR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17812897</td>\n",
       "      <td>HR MANAGER         Skill Highlights  ...</td>\n",
       "      <td>&lt;div class=\"fontsize fontface vmargins hmargin...</td>\n",
       "      <td>HR</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ID                                         Resume_str  \\\n",
       "0  16852973           HR ADMINISTRATOR/MARKETING ASSOCIATE\\...   \n",
       "1  22323967           HR SPECIALIST, US HR OPERATIONS      ...   \n",
       "2  33176873           HR DIRECTOR       Summary      Over 2...   \n",
       "3  27018550           HR SPECIALIST       Summary    Dedica...   \n",
       "4  17812897           HR MANAGER         Skill Highlights  ...   \n",
       "\n",
       "                                         Resume_html Category  \n",
       "0  <div class=\"fontsize fontface vmargins hmargin...       HR  \n",
       "1  <div class=\"fontsize fontface vmargins hmargin...       HR  \n",
       "2  <div class=\"fontsize fontface vmargins hmargin...       HR  \n",
       "3  <div class=\"fontsize fontface vmargins hmargin...       HR  \n",
       "4  <div class=\"fontsize fontface vmargins hmargin...       HR  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Specify the path to your CSV file\n",
    "file_location = R\"data/Resume.csv\"\n",
    "resume_data = pd.read_csv(file_location)\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "resume_data.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "404 Client Error: Not Found for url: https://raw.githubusercontent.com/RanjithJames/NLP_RESUME_FINAL_PROJECT/main/Skill_patterns.jsonl",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Retrieve the data from the URL\u001b[39;00m\n\u001b[0;32m      8\u001b[0m response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(json_url)\n\u001b[1;32m----> 9\u001b[0m \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Raises an exception for HTTP errors\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Convert JSON Lines string into a pandas DataFrame\u001b[39;00m\n\u001b[0;32m     12\u001b[0m skills_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_json(StringIO(response\u001b[38;5;241m.\u001b[39mtext), lines\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\ranji\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages\\requests\\models.py:1024\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1019\u001b[0m     http_error_msg \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1020\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Server Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreason\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for url: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1021\u001b[0m     )\n\u001b[0;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[1;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[1;31mHTTPError\u001b[0m: 404 Client Error: Not Found for url: https://raw.githubusercontent.com/RanjithJames/NLP_RESUME_FINAL_PROJECT/main/Skill_patterns.jsonl"
     ]
    }
   ],
   "source": [
    "\n",
    "import requests\n",
    "from io import StringIO\n",
    "\n",
    "# URL for the JSON Lines file\n",
    "json_url = 'https://raw.githubusercontent.com/RanjithJames/NLP_RESUME_FINAL_PROJECT/main/Skill_patterns.jsonl'\n",
    "\n",
    "# Retrieve the data from the URL\n",
    "response = requests.get(json_url)\n",
    "response.raise_for_status()  # Raises an exception for HTTP errors\n",
    "\n",
    "# Convert JSON Lines string into a pandas DataFrame\n",
    "skills_data = pd.read_json(StringIO(response.text), lines=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Download the spaCy model\n",
    "#!python -m spacy download en_core_web_lg\n",
    "\n",
    "#!python -m spacy download en_core_web_trf\n",
    "# !pip install spacy-transformers\n",
    "\n",
    "#!python -m spacy download en_core_web_md\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install spacy[cuda]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "# Enable GPU usage\n",
    "spacy.require_gpu()\n",
    "# Load the pre-trained spaCy model\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "# Initialize the matcher with the vocabulary\n",
    "skill_matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Add patterns to the matcher\n",
    "for idx, row in skills_data.iterrows():\n",
    "    skill_pattern = row['pattern']  # Assuming this is a list of dictionaries\n",
    "    skill_matcher.add(\"SKILL\", [skill_pattern])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install tqdm for progress tracking\n",
    "!pip install tqdm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def extract_skills_from_resumes(resume_texts):\n",
    "    extracted_data = []\n",
    "    for resume in tqdm(resume_texts, desc=\"Extracting skills\"):\n",
    "        doc = nlp(resume)\n",
    "        match_results = skill_matcher(doc)\n",
    "        skill_spans = [doc[start:end] for _, start, end in match_results]\n",
    "        entities = [(span.start_char, span.end_char, \"SKILL\") for span in skill_spans if span.text.strip()]\n",
    "        extracted_data.append((resume, {\"entities\": entities}))\n",
    "    return extracted_data\n",
    "\n",
    "# Limit the data to the first 5 resumes\n",
    "sample_resumes = resume_data['Resume_str'].tolist()\n",
    "\n",
    "# Extract skills from the sample resumes\n",
    "sample_training_data = extract_skills_from_resumes(sample_resumes)\n",
    "\n",
    "# Print the first item of the training data\n",
    "print(sample_training_data[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sample_training_data[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Save sample_training_data to a file\n",
    "# with open('sample_training_data.pkl', 'wb') as f:\n",
    "#     pickle.dump(sample_training_data, f)\n",
    "\n",
    "# Load sample_training_data from the file\n",
    "with open('sample_training_data.pkl', 'rb') as f:\n",
    "    sample_training_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy-lookups-data in c:\\users\\ranji\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages (1.0.5)\n",
      "Requirement already satisfied: setuptools in c:\\users\\ranji\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages (from spacy-lookups-data) (65.5.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Install scikit-learn for model training\n",
    "#!pip install scikit-learn\n",
    "!pip install spacy-lookups-data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Unzip the training data into texts and annotations\n",
    "texts, annotations = zip(*sample_training_data)\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "train_texts, val_texts, train_annotations, val_annotations = train_test_split(\n",
    "    texts, annotations, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "# Recreate the training and validation sets as tuples (text, annotation)\n",
    "train_data = list(zip(train_texts, train_annotations))\n",
    "val_data = list(zip(val_texts, val_annotations))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Add the directory to the PATH environment variable\n",
    "os.environ['PATH'] += os.pathsep + r'G:\\bin'\n",
    "\n",
    "# Verify the PATH has been updated\n",
    "print(os.environ['PATH'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # XX corresponds to your CUDA version, e.g., 117 for CUDA 11.7\n",
    "#!pip install spacy[cuda]\n",
    "!pip install cupy-cuda12x\n",
    "\n",
    "# Replace 120 with the closest supported CUDA version to yours\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ranji\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages\\cupy\\cuda\\compiler.py:233: PerformanceWarning: Jitify is performing a one-time only warm-up to populate the persistent cache, this may take a few seconds and will be improved in a future release...\n",
      "  jitify._init_module()\n",
      "Iteration 1: 100%|██████████| 1987/1987 [06:09<00:00,  5.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1 Losses: {'ner': 34599.23629094142}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration 2: 100%|██████████| 1987/1987 [06:04<00:00,  5.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2 Losses: {'ner': 8010.593752527938}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration 3: 100%|██████████| 1987/1987 [06:02<00:00,  5.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3 Losses: {'ner': 6272.731761513552}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration 4: 100%|██████████| 1987/1987 [06:01<00:00,  5.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4 Losses: {'ner': 5657.160444012684}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration 5: 100%|██████████| 1987/1987 [06:08<00:00,  5.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5 Losses: {'ner': 5088.763768329195}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration 6: 100%|██████████| 1987/1987 [06:27<00:00,  5.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6 Losses: {'ner': 4636.18936103732}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration 7: 100%|██████████| 1987/1987 [06:06<00:00,  5.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7 Losses: {'ner': 4347.569430024175}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration 8: 100%|██████████| 1987/1987 [06:15<00:00,  5.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 8 Losses: {'ner': 4089.385788787145}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration 9: 100%|██████████| 1987/1987 [06:15<00:00,  5.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 9 Losses: {'ner': 3769.045480025734}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration 10: 100%|██████████| 1987/1987 [06:15<00:00,  5.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10 Losses: {'ner': 3716.294557013825}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from spacy.tokens import Span\n",
    "from spacy.training import Example\n",
    "from spacy.lookups import load_lookups\n",
    "from spacy.matcher import Matcher\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "\n",
    "\n",
    "ner = nlp.get_pipe(\"ner\")\n",
    "\n",
    "# Function to remove overlapping entities\n",
    "def remove_overlapping_entities(entities):\n",
    "    \"\"\"\n",
    "    Remove overlapping entities from the list.\n",
    "    \n",
    "    Args:\n",
    "        entities (list): List of tuples containing start, end, and label of entities.\n",
    "    \n",
    "    Returns:\n",
    "        list: List of non-overlapping entities.\n",
    "    \"\"\"\n",
    "    entities = sorted(entities, key=lambda x: x[0])  # Sort by start position\n",
    "    non_overlapping_entities = []\n",
    "    last_end = -1\n",
    "    for start, end, label in entities:\n",
    "        if start >= last_end:  # No overlap\n",
    "            non_overlapping_entities.append((start, end, label))\n",
    "            last_end = end\n",
    "    return non_overlapping_entities\n",
    "\n",
    "# Add the 'SKILL' entity label to the NER model if it's not already known\n",
    "if 'SKILL' not in ner.labels:\n",
    "    ner.add_label(\"SKILL\")\n",
    "\n",
    "# Function to add custom entity labels from training data\n",
    "def add_custom_entity_labels(train_data):\n",
    "    \"\"\"\n",
    "    Add custom entity labels from training data to the NER model.\n",
    "    \n",
    "    Args:\n",
    "        train_data (list): List of tuples containing text and annotations.\n",
    "    \"\"\"\n",
    "    for _, annotations in train_data:\n",
    "        for ent in annotations.get(\"entities\"):\n",
    "            if ent[2] not in ner.labels:\n",
    "                ner.add_label(ent[2])\n",
    "\n",
    "# Add custom entities from training data\n",
    "add_custom_entity_labels(train_data)\n",
    "\n",
    "# Disable other pipeline components during training to train only NER\n",
    "unaffected_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
    "\n",
    "# Function to preprocess training data to remove overlapping entities\n",
    "def preprocess_training_data(train_data):\n",
    "    \"\"\"\n",
    "    Preprocess the training data to remove overlapping entities.\n",
    "    \n",
    "    Args:\n",
    "        train_data (list): List of tuples containing text and annotations.\n",
    "    \n",
    "    Returns:\n",
    "        list: List of preprocessed training data.\n",
    "    \"\"\"\n",
    "    preprocessed_data = []\n",
    "    for text, annotations in train_data:\n",
    "        entities = annotations.get(\"entities\")\n",
    "        non_overlapping_entities = remove_overlapping_entities(entities)\n",
    "        preprocessed_data.append((text, {\"entities\": non_overlapping_entities}))\n",
    "    return preprocessed_data\n",
    "\n",
    "# Preprocess training data to remove overlapping entities\n",
    "train_data = preprocess_training_data(train_data)\n",
    "\n",
    "# Function to train NER model\n",
    "def train_ner_model(train_data, n_iter=10):\n",
    "    \"\"\"\n",
    "    Train the NER model with the given training data.\n",
    "    \n",
    "    Args:\n",
    "        train_data (list): List of tuples containing text and annotations.\n",
    "        n_iter (int): Number of training iterations.\n",
    "    \"\"\"\n",
    "    with nlp.disable_pipes(*unaffected_pipes):  # Only train NER\n",
    "        optimizer = nlp.initialize()  # Correct initialization for transformer-based models\n",
    "        for itn in range(n_iter):\n",
    "            random.shuffle(train_data)\n",
    "            losses = {}\n",
    "            for text, annotations in tqdm(train_data, desc=f\"Iteration {itn+1}\"):\n",
    "                example = Example.from_dict(nlp.make_doc(text), annotations)\n",
    "                nlp.update([example], sgd=optimizer, drop=0.5, losses=losses)\n",
    "            print(f\"Iteration {itn+1} Losses: {losses}\")\n",
    "\n",
    "# Train the NER model\n",
    "train_ner_model(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import spacy\n",
    "from spacy.tokens import Span\n",
    "from spacy.training import Example\n",
    "from spacy.scorer import Scorer\n",
    "\n",
    "# Define a valid path for saving the model\n",
    "model_dir = r\"data/model\"\n",
    "\n",
    "# Create the directory if it does not exist\n",
    "if not os.path.exists(model_dir):\n",
    "    os.makedirs(model_dir)\n",
    "\n",
    "# Save the trained model to disk\n",
    "def save_model(nlp, model_dir):\n",
    "    \"\"\"\n",
    "    Save the trained spaCy model to disk.\n",
    "    \n",
    "    Args:\n",
    "        nlp (Language): The spaCy model to be saved.\n",
    "        model_dir (str): The directory path where the model will be saved.\n",
    "    \"\"\"\n",
    "    nlp.to_disk(model_dir)\n",
    "\n",
    "save_model(nlp, model_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "business SKILL\n",
      "play SKILL\n",
      "analytics SKILL\n",
      "business SKILL\n",
      "algorithms SKILL\n",
      "Data Analysis SKILL\n",
      "data analysis SKILL\n",
      "support SKILL\n",
      "business SKILL\n",
      "algorithms SKILL\n",
      "business SKILL\n",
      "scalability SKILL\n",
      "Collaboration SKILL\n",
      "business SKILL\n",
      "Data Visualization SKILL\n",
      "support SKILL\n",
      "data science SKILL\n",
      "AI SKILL\n",
      "Computer Science SKILL\n",
      "data science SKILL\n",
      "languages SKILL\n",
      "Python SKILL\n",
      "R SKILL\n",
      "SQL SKILL\n",
      "libraries SKILL\n",
      "TensorFlow SKILL\n",
      "data mining SKILL\n",
      "data visualization SKILL\n",
      "Tableau SKILL\n",
      "Hadoop SKILL\n",
      "engineering SKILL\n",
      "testing SKILL\n",
      "design SKILL\n",
      "schedule SKILL\n"
     ]
    }
   ],
   "source": [
    "# Load the model from the saved directory into a new variable\n",
    "\n",
    "\n",
    "def load_model(model_dir):\n",
    "    \"\"\"\n",
    "    Load a spaCy model from a specified directory.\n",
    "    \n",
    "    Args:\n",
    "        model_dir (str): The directory path from where the model will be loaded.\n",
    "    \n",
    "    Returns:\n",
    "        Language: The loaded spaCy model.\n",
    "    \"\"\"\n",
    "    return spacy.load(model_dir)\n",
    "\n",
    "newnlp = load_model('data/model')\n",
    "\n",
    "\n",
    "# Process a new text through the loaded model\n",
    "def print_entities(ner_model, text):\n",
    "    \"\"\"\n",
    "    Process the text through the NER model and print the recognized entities and their labels.\n",
    "    \n",
    "    Args:\n",
    "        ner_model (Language): The spaCy NER model.\n",
    "        text (str): The text to be processed.\n",
    "    \"\"\"\n",
    "    doc = ner_model(text)\n",
    "    for ent in doc.ents:\n",
    "        print(ent.text, ent.label_)\n",
    "\n",
    "# Example text to process\n",
    "test_text = \"\"\"### Data Scientist Job Description\n",
    "\n",
    "**Job Title:** Data Scientist\n",
    "\n",
    "**Location:** [City, State]\n",
    "\n",
    "**Company:** [Company Name]\n",
    "\n",
    "**Job Type:** Full-time\n",
    "\n",
    "**About Us:**\n",
    "[Company Name] is a leading [industry] company, dedicated to [mission statement or company goal]. We leverage cutting-edge technologies and data-driven strategies to drive innovation and make informed decisions that impact our business and customers. We are seeking a passionate and analytical Data Scientist to join our dynamic team.\n",
    "\n",
    "**Job Summary:**\n",
    "As a Data Scientist at [Company Name], you will play a key role in driving data-driven decision-making by developing, deploying, and maintaining machine learning models and advanced analytics solutions. You will work closely with cross-functional teams to analyze large datasets, uncover insights, and solve complex business problems. This role requires a deep understanding of statistical methods, machine learning algorithms, and data manipulation techniques.\n",
    "\n",
    "**Key Responsibilities:**\n",
    "- **Data Analysis & Exploration:**\n",
    "  - Collect, process, and analyze large datasets from various sources to identify trends, patterns, and anomalies.\n",
    "  - Perform exploratory data analysis (EDA) to generate insights and support business decisions.\n",
    "  \n",
    "- **Model Development:**\n",
    "  - Develop, test, and deploy predictive models and machine learning algorithms to solve business problems.\n",
    "  - Optimize models for performance, scalability, and interpretability.\n",
    "  \n",
    "- **Collaboration & Communication:**\n",
    "  - Work closely with business stakeholders to understand their needs and translate them into analytical solutions.\n",
    "  - Communicate complex analytical concepts and insights to non-technical stakeholders in a clear and concise manner.\n",
    "\n",
    "- **Data Visualization:**\n",
    "  - Create interactive dashboards and visualizations to present data insights and trends.\n",
    "  - Provide recommendations based on data-driven insights to support decision-making.\n",
    "\n",
    "- **Continuous Improvement:**\n",
    "  - Stay up-to-date with the latest advancements in data science, machine learning, and AI technologies.\n",
    "  - Identify opportunities to improve data processes and contribute to the development of data-driven strategies.\n",
    "\n",
    "**Qualifications:**\n",
    "- Bachelor’s degree in Computer Science, Statistics, Mathematics, or a related field; Master’s or PhD preferred.\n",
    "- 2+ years of experience in data science, machine learning, or a related field.\n",
    "- Proficiency in programming languages such as Python, R, or SQL.\n",
    "- Experience with machine learning frameworks and libraries such as TensorFlow, scikit-learn, or PyTorch.\n",
    "- Strong knowledge of statistical methods, data mining, and predictive modeling techniques.\n",
    "- Experience with data visualization tools like Tableau, Power BI, or matplotlib.\n",
    "- Familiarity with big data tools and platforms such as Hadoop, Spark, or AWS.\n",
    "- Excellent problem-solving skills and attention to detail.\n",
    "- Strong communication skills and ability to work collaboratively in a team environment.\n",
    "\n",
    "**Preferred Skills:**\n",
    "- Experience with natural language processing (NLP) or deep learning.\n",
    "- Knowledge of cloud computing and data engineering.\n",
    "- Experience with A/B testing and experiment design.\n",
    "\n",
    "**Benefits:**\n",
    "- Competitive salary and performance-based bonuses.\n",
    "- Health, dental, and vision insurance.\n",
    "- 401(k) plan with company match.\n",
    "- Generous paid time off and holiday schedule.\n",
    "- Opportunities for professional development and career growth.\n",
    "\n",
    "**How to Apply:**\n",
    "Interested candidates should submit their resume, cover letter, and portfolio of relevant work to [email address] with the subject line “Data Scientist Application – [Your Name]”.\n",
    "\n",
    "[Company Name] is an equal opportunity employer. We celebrate diversity and are committed to creating an inclusive environment for all employees.\"\"\"\n",
    "# Print recognized entities and their labels\n",
    "print_entities(newnlp, test_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "from huggingface_hub import Repository, login\n",
    "\n",
    "def save_model_to_huggingface(nlp, model_dir, repo_id, token):\n",
    "    \"\"\"\n",
    "    Save the trained spaCy model to Hugging Face Model Hub by cloning to a new folder and moving files.\n",
    "    \n",
    "    Args:\n",
    "        nlp (Language): The spaCy model to be saved.\n",
    "        model_dir (str): The directory path where the model will be saved.\n",
    "        repo_id (str): The repository ID on Hugging Face (e.g., 'Ranjithjames/SPACY_NER').\n",
    "        token (str): Your Hugging Face API token.\n",
    "    \"\"\"\n",
    "    # Save the spaCy model to the specified directory\n",
    "    nlp.to_disk(model_dir)\n",
    "\n",
    "    # Authenticate the Hugging Face Hub\n",
    "    login(token=token)\n",
    "    \n",
    "    # Temporary directory for cloning the repository\n",
    "    temp_model_dir = model_dir + \"_temp\"\n",
    "    \n",
    "    # Clone the repository to the temporary directory\n",
    "    repo_url = f\"https://huggingface.co/{repo_id}\"\n",
    "    repo = Repository(local_dir=temp_model_dir, clone_from=repo_url, use_auth_token=token)\n",
    "    \n",
    "    # Move the saved model files to the cloned repository directory\n",
    "    for item in os.listdir(model_dir):\n",
    "        s = os.path.join(model_dir, item)\n",
    "        d = os.path.join(temp_model_dir, item)\n",
    "        if os.path.isdir(s):\n",
    "            shutil.move(s, d)\n",
    "        else:\n",
    "            shutil.move(s, d)\n",
    "    \n",
    "    # Add and push the model files to the repository\n",
    "    repo.git_add(auto_lfs_track=True)\n",
    "    repo.git_commit(\"Update spaCy model\")\n",
    "    repo.git_push()\n",
    "    \n",
    "    # Clean up: move files back to original directory and remove temporary directory\n",
    "    for item in os.listdir(temp_model_dir):\n",
    "        s = os.path.join(temp_model_dir, item)\n",
    "        d = os.path.join(model_dir, item)\n",
    "        if os.path.isdir(s):\n",
    "            shutil.move(s, d)\n",
    "        else:\n",
    "            shutil.move(s, d)\n",
    "    shutil.rmtree(temp_model_dir)\n",
    "\n",
    "# Example usage\n",
    "model_dir = \"data/model\"\n",
    "repo_id = \"Ranjithjames/SPACY_NER\"\n",
    "token = \"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\"\n",
    "\n",
    "save_model_to_huggingface(newnlp, model_dir, repo_id, token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "def download_model_from_huggingface(repo_id, model_dir, token):\n",
    "    \"\"\"\n",
    "    Download the model from Hugging Face Model Hub and store it in the specified directory.\n",
    "    \n",
    "    Args:\n",
    "        repo_id (str): The repository ID on Hugging Face (e.g., 'Ranjithjames/SPACY_NER').\n",
    "        model_dir (str): The directory path where the model will be stored.\n",
    "        token (str): Your Hugging Face API token.\n",
    "    \"\"\"\n",
    "    # Create the model directory if it doesn't exist\n",
    "    if not os.path.exists(model_dir):\n",
    "        os.makedirs(model_dir)\n",
    "    \n",
    "    # Download the model files from the Hugging Face repository\n",
    "    files_to_download = [\"config.json\", \"tokenizer.json\", \"model.bin\", \"special_tokens_map.json\", \"vocab.txt\"]\n",
    "    \n",
    "    for file_name in files_to_download:\n",
    "        file_path = hf_hub_download(repo_id=repo_id, filename=file_name, use_auth_token=token)\n",
    "        shutil.move(file_path, os.path.join(model_dir, file_name))\n",
    "        \n",
    "# Example usage\n",
    "model_dir = \"data/model\"\n",
    "repo_id = \"Ranjithjames/SPACY_NER\"\n",
    "token = \"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\"\n",
    "\n",
    "download_model_from_huggingface(repo_id, model_dir, token)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 497/497 [00:51<00:00,  9.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9986\n",
      "Recall: 0.9987\n",
      "F1 Score: 0.9986\n",
      "Support: None\n",
      "Confusion Matrix:\n",
      "[[   907      0      0    220      0]\n",
      " [     0    907      0    184     36]\n",
      " [     2      2     12     27      6]\n",
      " [    74     54      1 539517     47]\n",
      " [     0     20      0     63  11232]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     B-SKILL       0.92      0.80      0.86      1127\n",
      "     L-SKILL       0.92      0.80      0.86      1127\n",
      "     I-SKILL       0.92      0.24      0.39        49\n",
      "           O       1.00      1.00      1.00    539693\n",
      "     U-SKILL       0.99      0.99      0.99     11315\n",
      "\n",
      "    accuracy                           1.00    553311\n",
      "   macro avg       0.95      0.77      0.82    553311\n",
      "weighted avg       1.00      1.00      1.00    553311\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix, classification_report\n",
    "from tqdm import tqdm\n",
    "from spacy.training import offsets_to_biluo_tags, biluo_tags_to_offsets\n",
    "import numpy as np\n",
    "\n",
    "# Load the model\n",
    "def load_model(model_dir):\n",
    "    \"\"\"\n",
    "    Load a spaCy model from a specified directory.\n",
    "    \n",
    "    Args:\n",
    "        model_dir (str): The directory path from where the model will be loaded.\n",
    "    \n",
    "    Returns:\n",
    "        Language: The loaded spaCy model.\n",
    "    \"\"\"\n",
    "    return spacy.load(model_dir)\n",
    "\n",
    "newnlp = load_model('data/model')\n",
    "\n",
    "def non_overlapping_entities(entities):\n",
    "    \"\"\"\n",
    "    Ensure entities are non-overlapping by sorting and filtering.\n",
    "    \n",
    "    Args:\n",
    "        entities (list of tuples): List of entities (start, end, label).\n",
    "    \n",
    "    Returns:\n",
    "        list of tuples: Filtered non-overlapping entities.\n",
    "    \"\"\"\n",
    "    sorted_entities = sorted(entities, key=lambda x: x[0])\n",
    "    non_overlapping = []\n",
    "\n",
    "    for ent in sorted_entities:\n",
    "        if not non_overlapping or ent[0] >= non_overlapping[-1][1]:\n",
    "            non_overlapping.append(ent)\n",
    "    return non_overlapping\n",
    "\n",
    "# Function to evaluate the model with progress tracking\n",
    "def evaluate_model(ner_model, test_data):\n",
    "    \"\"\"\n",
    "    Evaluate the NER model using the test data and calculate precision, recall, F1 scores, and support.\n",
    "    \n",
    "    Args:\n",
    "        ner_model (Language): The spaCy NER model.\n",
    "        test_data (list of tuples): A list where each tuple is of the form (text, {'entities': [(start, end, label), ...]})\n",
    "    \n",
    "    Returns:\n",
    "        dict: A dictionary containing precision, recall, F1 scores, support, and confusion matrix.\n",
    "    \"\"\"\n",
    "    true_entities = []\n",
    "    pred_entities = []\n",
    "\n",
    "    for text, annotations in tqdm(test_data, desc=\"Evaluating\"):\n",
    "        doc = ner_model(text)\n",
    "        true_ents = non_overlapping_entities(annotations['entities'])\n",
    "        pred_ents = [(ent.start_char, ent.end_char, ent.label_) for ent in doc.ents]\n",
    "        \n",
    "        # Generate token-based BIO tags for the true entities\n",
    "        true_tags = offsets_to_biluo_tags(ner_model.make_doc(text), true_ents)\n",
    "        \n",
    "        # Generate token-based BIO tags for the predicted entities\n",
    "        pred_tags = offsets_to_biluo_tags(ner_model.make_doc(text), pred_ents)\n",
    "        \n",
    "        # Append to the lists\n",
    "        true_entities.extend(true_tags)\n",
    "        pred_entities.extend(pred_tags)\n",
    "\n",
    "    # Calculate precision, recall, f1, and support\n",
    "    precision, recall, f1, support = precision_recall_fscore_support(true_entities, pred_entities, average='weighted')\n",
    "\n",
    "    # Generate confusion matrix\n",
    "    labels = list(set(true_entities + pred_entities))\n",
    "    conf_matrix = confusion_matrix(true_entities, pred_entities, labels=labels)\n",
    "    \n",
    "    # Generate classification report\n",
    "    class_report = classification_report(true_entities, pred_entities, labels=labels)\n",
    "\n",
    "    return {\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'support': support,\n",
    "        'confusion_matrix': conf_matrix,\n",
    "        'classification_report': class_report\n",
    "    }\n",
    "\n",
    "\n",
    "# Evaluate the model with progress tracking\n",
    "evaluation_results = evaluate_model(newnlp, val_data)\n",
    "\n",
    "print(\"Precision: {:.4f}\".format(evaluation_results['precision']))\n",
    "print(\"Recall: {:.4f}\".format(evaluation_results['recall']))\n",
    "print(\"F1 Score: {:.4f}\".format(evaluation_results['f1']))\n",
    "print(\"Support: {}\".format(evaluation_results['support']))\n",
    "print(\"Confusion Matrix:\\n{}\".format(evaluation_results['confusion_matrix']))\n",
    "print(\"Classification Report:\\n{}\".format(evaluation_results['classification_report']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
