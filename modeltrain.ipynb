{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Resume_str</th>\n",
       "      <th>Resume_html</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16852973</td>\n",
       "      <td>HR ADMINISTRATOR/MARKETING ASSOCIATE\\...</td>\n",
       "      <td>&lt;div class=\"fontsize fontface vmargins hmargin...</td>\n",
       "      <td>HR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>22323967</td>\n",
       "      <td>HR SPECIALIST, US HR OPERATIONS      ...</td>\n",
       "      <td>&lt;div class=\"fontsize fontface vmargins hmargin...</td>\n",
       "      <td>HR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>33176873</td>\n",
       "      <td>HR DIRECTOR       Summary      Over 2...</td>\n",
       "      <td>&lt;div class=\"fontsize fontface vmargins hmargin...</td>\n",
       "      <td>HR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27018550</td>\n",
       "      <td>HR SPECIALIST       Summary    Dedica...</td>\n",
       "      <td>&lt;div class=\"fontsize fontface vmargins hmargin...</td>\n",
       "      <td>HR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17812897</td>\n",
       "      <td>HR MANAGER         Skill Highlights  ...</td>\n",
       "      <td>&lt;div class=\"fontsize fontface vmargins hmargin...</td>\n",
       "      <td>HR</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ID                                         Resume_str  \\\n",
       "0  16852973           HR ADMINISTRATOR/MARKETING ASSOCIATE\\...   \n",
       "1  22323967           HR SPECIALIST, US HR OPERATIONS      ...   \n",
       "2  33176873           HR DIRECTOR       Summary      Over 2...   \n",
       "3  27018550           HR SPECIALIST       Summary    Dedica...   \n",
       "4  17812897           HR MANAGER         Skill Highlights  ...   \n",
       "\n",
       "                                         Resume_html Category  \n",
       "0  <div class=\"fontsize fontface vmargins hmargin...       HR  \n",
       "1  <div class=\"fontsize fontface vmargins hmargin...       HR  \n",
       "2  <div class=\"fontsize fontface vmargins hmargin...       HR  \n",
       "3  <div class=\"fontsize fontface vmargins hmargin...       HR  \n",
       "4  <div class=\"fontsize fontface vmargins hmargin...       HR  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Specify the path to your CSV file\n",
    "file_location = R\"data/Resume.csv\"\n",
    "resume_data = pd.read_csv(file_location)\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "resume_data.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import requests\n",
    "from io import StringIO\n",
    "\n",
    "# URL for the JSON Lines file\n",
    "json_url = 'https://raw.githubusercontent.com/mystery2life/NLP-Project/main/temp_patterns.jsonl'\n",
    "\n",
    "# Retrieve the data from the URL\n",
    "response = requests.get(json_url)\n",
    "response.raise_for_status()  # Raises an exception for HTTP errors\n",
    "\n",
    "# Convert JSON Lines string into a pandas DataFrame\n",
    "skills_data = pd.read_json(StringIO(response.text), lines=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Download the spaCy model\n",
    "#!python -m spacy download en_core_web_lg\n",
    "\n",
    "#!python -m spacy download en_core_web_trf\n",
    "# !pip install spacy-transformers\n",
    "\n",
    "# !python -m spacy download en_core_web_md\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the pre-trained spaCy model\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "# Initialize the matcher with the vocabulary\n",
    "skill_matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Add patterns to the matcher\n",
    "for idx, row in skills_data.iterrows():\n",
    "    skill_pattern = row['pattern']  # Assuming this is a list of dictionaries\n",
    "    skill_matcher.add(\"SKILL\", [skill_pattern])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install tqdm for progress tracking\n",
    "!pip install tqdm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def extract_skills_from_resumes(resume_texts):\n",
    "    extracted_data = []\n",
    "    for resume in tqdm(resume_texts, desc=\"Extracting skills\"):\n",
    "        doc = nlp(resume)\n",
    "        match_results = skill_matcher(doc)\n",
    "        skill_spans = [doc[start:end] for _, start, end in match_results]\n",
    "        entities = [(span.start_char, span.end_char, \"SKILL\") for span in skill_spans if span.text.strip()]\n",
    "        extracted_data.append((resume, {\"entities\": entities}))\n",
    "    return extracted_data\n",
    "\n",
    "# Limit the data to the first 5 resumes\n",
    "sample_resumes = resume_data['Resume_str'].tolist()\n",
    "\n",
    "# Extract skills from the sample resumes\n",
    "sample_training_data = extract_skills_from_resumes(sample_resumes)\n",
    "\n",
    "# Print the first item of the training data\n",
    "print(sample_training_data[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Save sample_training_data to a file\n",
    "# with open('sample_training_data.pkl', 'wb') as f:\n",
    "#     pickle.dump(sample_training_data, f)\n",
    "\n",
    "# Load sample_training_data from the file\n",
    "with open('sample_training_data.pkl', 'rb') as f:\n",
    "    sample_training_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Install scikit-learn for model training\n",
    "#!pip install scikit-learn\n",
    "!pip install spacy-lookups-data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Unzip the training data into texts and annotations\n",
    "texts, annotations = zip(*sample_training_data)\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "train_texts, val_texts, train_annotations, val_annotations = train_test_split(\n",
    "    texts, annotations, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "# Recreate the training and validation sets as tuples (text, annotation)\n",
    "train_data = list(zip(train_texts, train_annotations))\n",
    "val_data = list(zip(val_texts, val_annotations))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from spacy.tokens import Span\n",
    "from spacy.training import Example\n",
    "from spacy.lookups import load_lookups\n",
    "from spacy.matcher import Matcher\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "\n",
    "\n",
    "ner = nlp.get_pipe(\"ner\")\n",
    "\n",
    "# Function to remove overlapping entities\n",
    "def remove_overlapping_entities(entities):\n",
    "    \"\"\"\n",
    "    Remove overlapping entities from the list.\n",
    "    \n",
    "    Args:\n",
    "        entities (list): List of tuples containing start, end, and label of entities.\n",
    "    \n",
    "    Returns:\n",
    "        list: List of non-overlapping entities.\n",
    "    \"\"\"\n",
    "    entities = sorted(entities, key=lambda x: x[0])  # Sort by start position\n",
    "    non_overlapping_entities = []\n",
    "    last_end = -1\n",
    "    for start, end, label in entities:\n",
    "        if start >= last_end:  # No overlap\n",
    "            non_overlapping_entities.append((start, end, label))\n",
    "            last_end = end\n",
    "    return non_overlapping_entities\n",
    "\n",
    "# Add the 'SKILL' entity label to the NER model if it's not already known\n",
    "if 'SKILL' not in ner.labels:\n",
    "    ner.add_label(\"SKILL\")\n",
    "\n",
    "# Function to add custom entity labels from training data\n",
    "def add_custom_entity_labels(train_data):\n",
    "    \"\"\"\n",
    "    Add custom entity labels from training data to the NER model.\n",
    "    \n",
    "    Args:\n",
    "        train_data (list): List of tuples containing text and annotations.\n",
    "    \"\"\"\n",
    "    for _, annotations in train_data:\n",
    "        for ent in annotations.get(\"entities\"):\n",
    "            if ent[2] not in ner.labels:\n",
    "                ner.add_label(ent[2])\n",
    "\n",
    "# Add custom entities from training data\n",
    "add_custom_entity_labels(train_data)\n",
    "\n",
    "# Disable other pipeline components during training to train only NER\n",
    "unaffected_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
    "\n",
    "# Function to preprocess training data to remove overlapping entities\n",
    "def preprocess_training_data(train_data):\n",
    "    \"\"\"\n",
    "    Preprocess the training data to remove overlapping entities.\n",
    "    \n",
    "    Args:\n",
    "        train_data (list): List of tuples containing text and annotations.\n",
    "    \n",
    "    Returns:\n",
    "        list: List of preprocessed training data.\n",
    "    \"\"\"\n",
    "    preprocessed_data = []\n",
    "    for text, annotations in train_data:\n",
    "        entities = annotations.get(\"entities\")\n",
    "        non_overlapping_entities = remove_overlapping_entities(entities)\n",
    "        preprocessed_data.append((text, {\"entities\": non_overlapping_entities}))\n",
    "    return preprocessed_data\n",
    "\n",
    "# Preprocess training data to remove overlapping entities\n",
    "train_data = preprocess_training_data(train_data)\n",
    "\n",
    "# Function to train NER model\n",
    "def train_ner_model(train_data, n_iter=10):\n",
    "    \"\"\"\n",
    "    Train the NER model with the given training data.\n",
    "    \n",
    "    Args:\n",
    "        train_data (list): List of tuples containing text and annotations.\n",
    "        n_iter (int): Number of training iterations.\n",
    "    \"\"\"\n",
    "    with nlp.disable_pipes(*unaffected_pipes):  # Only train NER\n",
    "        optimizer = nlp.initialize()  # Correct initialization for transformer-based models\n",
    "        for itn in range(n_iter):\n",
    "            random.shuffle(train_data)\n",
    "            losses = {}\n",
    "            for text, annotations in tqdm(train_data, desc=f\"Iteration {itn+1}\"):\n",
    "                example = Example.from_dict(nlp.make_doc(text), annotations)\n",
    "                nlp.update([example], sgd=optimizer, drop=0.5, losses=losses)\n",
    "            print(f\"Iteration {itn+1} Losses: {losses}\")\n",
    "\n",
    "# Train the NER model\n",
    "train_ner_model(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import spacy\n",
    "from spacy.tokens import Span\n",
    "from spacy.training import Example\n",
    "from spacy.scorer import Scorer\n",
    "\n",
    "# Define a valid path for saving the model\n",
    "model_dir = r\"/Users/RanjithJames/Downloads/NLP-Project-project/data/model\"\n",
    "\n",
    "# Create the directory if it does not exist\n",
    "if not os.path.exists(model_dir):\n",
    "    os.makedirs(model_dir)\n",
    "\n",
    "# Save the trained model to disk\n",
    "def save_model(nlp, model_dir):\n",
    "    \"\"\"\n",
    "    Save the trained spaCy model to disk.\n",
    "    \n",
    "    Args:\n",
    "        nlp (Language): The spaCy model to be saved.\n",
    "        model_dir (str): The directory path where the model will be saved.\n",
    "    \"\"\"\n",
    "    nlp.to_disk(model_dir)\n",
    "\n",
    "save_model(nlp, model_dir)\n",
    "\n",
    "# Load the model from the saved directory into a new variable\n",
    "def load_model(model_dir):\n",
    "    \"\"\"\n",
    "    Load a spaCy model from a specified directory.\n",
    "    \n",
    "    Args:\n",
    "        model_dir (str): The directory path from where the model will be loaded.\n",
    "    \n",
    "    Returns:\n",
    "        Language: The loaded spaCy model.\n",
    "    \"\"\"\n",
    "    return spacy.load(model_dir)\n",
    "\n",
    "newnlp = load_model(model_dir)\n",
    "\n",
    "\n",
    "# Process a new text through the loaded model\n",
    "def print_entities(ner_model, text):\n",
    "    \"\"\"\n",
    "    Process the text through the NER model and print the recognized entities and their labels.\n",
    "    \n",
    "    Args:\n",
    "        ner_model (Language): The spaCy NER model.\n",
    "        text (str): The text to be processed.\n",
    "    \"\"\"\n",
    "    doc = ner_model(text)\n",
    "    for ent in doc.ents:\n",
    "        print(ent.text, ent.label_)\n",
    "\n",
    "# Example text to process\n",
    "test_text = \"Abid Ali Awan Data Scientist I am a certified data scientist professional, who loves building machine learning models and blogs about the latest AI technologies. I am currently testing AI Products at PEC-PITC, which later gets approved for human trials. abidaliawan@tutamail.com +923456855126 Islamabad, Pakistan abidaliawan.me WORK EXPERIENCE Data Scientist Pakistan Innovation and Testing Center - PEC 04/2021 - Present, Islamabad, Pakistan Redesigned data of engineers that were mostly scattered and unavailable. Designed dashboard and data analysis report to help higher management make better decisions. Accessibility of key information has created a new culture of making data-driven decisions. Contact: Ali Raza Asif - darkslayerraza10@gmail.com Data Scientist Freelancing/Kaggle 11/2020 - Present, Islamabad, Pakistan Engineered a healthcare system. Used machine learning to detect some of the common decisions. The project has paved the way for others to use new techniques to get better results. Participated in Kaggle machine learning competitions. Learned new techniques to get a better score and finally got to 1 percent rank. Researcher / Event Organizer CREDIT 02/2017 - 07/2017, Kuala Lumpur, Malaysia Marketing for newly build research lab. Organized technical events and successfully invited the multiple company's CEO for talks. Reduced the gap between industries and educational institutes. Research on new development in the IoT sector. Created research proposal for funding. Investigated the new communication protocol for IoT devices. Contact: Dr. Tan Chye Cheah - dr.chyecheah.t@apu.edu.my EDUCATION MSc in Technology Management Staffordshire University 11/2015 - 04/2017, Postgraduate with Distinction Challenges in Implementing IoT-enabled Smart cities in Malaysia. Bachelors Electrical Telecommunication Engineering COMSATS Institute of Information Technology, Islamabad 08/2010 - 01/2014, CGPA: 3.09 Networking Satellite communications Programming/ Matlab Telecommunication Engineering SKILLS Designing Leadership Media/Marketing R/Python SQL Tableau NLP Data Analysis Machine learning Deep learning Webapp/Cloud Feature Engineering Ensembling Time Series Technology Management ACHIEVEMENTS 98th Hungry Geese Simulation Competition (08/2021) 2nd in Covid-19 vaccinations around the world (07/2021) 8th in Automatic Speech Recognition in WOLOF (06/2021) Top 10 in WiDS Datathon. (03/2021) 40th / 622 in MagNet: Model the Geomagnetic Field Hosted by NOAA (02/2021) 18th in Rock, Paper, Scissors/Designing AI Agent Competition. (02/2021) PROJECTS Goodreads Profile Analysis WebApp (09/2021) Data Analysis Web Scraping XLM Interactive Visualization Contributed in orchest.io (08/2021) Testing and Debuging Technical Article Proposing new was to Improve ML pipelines World Vaccine Update System (06/2021) Used sqlite3 for database Automated system for daily update the Kaggle DB and Analysis Interactive dashboard mRNA-Vaccine-Degradation-Prediction (06/2021) Explore our dataset and then preprocessed sequence, structure, and predicted loop type features Train deep learning GRU model Trip Advisor Data Analysis/ML (04/2021) Preprocessing Data, Exploratory Data analysis, Word clouds. Feature Engineering, Text processing. BiLSTM Model for predicting rating, evaluation, model performance. Jane Street Market Prediction (03/2021) EDA, Feature Engineering, experimenting with hyperparameters. Ensembling: Resnet, NN Embeddings, TF Simple NN model. Using simple MLP pytorch model. Achievements/Tasks Achievements/Tasks Achievements/Tasks Thesis Courses\"\n",
    "# Print recognized entities and their labels\n",
    "print_entities(newnlp, test_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI SKILL\n",
      "testing SKILL\n",
      "AI SKILL\n",
      "Testing SKILL\n",
      "data analysis SKILL\n",
      "machine learning SKILL\n",
      "Marketing SKILL\n",
      "Engineering SKILL\n",
      "communications SKILL\n",
      "Engineering SKILL\n",
      "Marketing SKILL\n",
      "R SKILL\n",
      "Python SKILL\n",
      "SQL SKILL\n",
      "Tableau SKILL\n",
      "NLP SKILL\n",
      "Data Analysis SKILL\n",
      "Engineering SKILL\n",
      "Time Series SKILL\n",
      "Simulation SKILL\n",
      "AI SKILL\n",
      "Data Analysis SKILL\n",
      "Visualization SKILL\n",
      "Testing SKILL\n",
      "ML SKILL\n",
      "database SKILL\n",
      "Data Analysis SKILL\n",
      "ML SKILL\n",
      "Data analysis SKILL\n",
      "Engineering SKILL\n",
      "Engineering SKILL\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import spacy\n",
    "from spacy.tokens import Span\n",
    "from spacy.training import Example\n",
    "from spacy.scorer import Scorer\n",
    "\n",
    "def load_model(model_dir):\n",
    "    \"\"\"\n",
    "    Load a spaCy model from a specified directory.\n",
    "    \n",
    "    Args:\n",
    "        model_dir (str): The directory path from where the model will be loaded.\n",
    "    \n",
    "    Returns:\n",
    "        Language: The loaded spaCy model.\n",
    "    \"\"\"\n",
    "    return spacy.load(model_dir)\n",
    "newnlp = load_model(model_dir)\n",
    "\n",
    "\n",
    "# Process a new text through the loaded model\n",
    "def print_entities(ner_model, text):\n",
    "    \"\"\"\n",
    "    Process the text through the NER model and print the recognized entities and their labels.\n",
    "    \n",
    "    Args:\n",
    "        ner_model (Language): The spaCy NER model.\n",
    "        text (str): The text to be processed.\n",
    "    \"\"\"\n",
    "    doc = ner_model(text)\n",
    "    for ent in doc.ents:\n",
    "        print(ent.text, ent.label_)\n",
    "\n",
    "# Example text to process\n",
    "test_text = \"Abid Ali Awan Data Scientist I am a certified data scientist professional, who loves building machine learning models and blogs about the latest AI technologies. I am currently testing AI Products at PEC-PITC, which later gets approved for human trials. abidaliawan@tutamail.com +923456855126 Islamabad, Pakistan abidaliawan.me WORK EXPERIENCE Data Scientist Pakistan Innovation and Testing Center - PEC 04/2021 - Present, Islamabad, Pakistan Redesigned data of engineers that were mostly scattered and unavailable. Designed dashboard and data analysis report to help higher management make better decisions. Accessibility of key information has created a new culture of making data-driven decisions. Contact: Ali Raza Asif - darkslayerraza10@gmail.com Data Scientist Freelancing/Kaggle 11/2020 - Present, Islamabad, Pakistan Engineered a healthcare system. Used machine learning to detect some of the common decisions. The project has paved the way for others to use new techniques to get better results. Participated in Kaggle machine learning competitions. Learned new techniques to get a better score and finally got to 1 percent rank. Researcher / Event Organizer CREDIT 02/2017 - 07/2017, Kuala Lumpur, Malaysia Marketing for newly build research lab. Organized technical events and successfully invited the multiple company's CEO for talks. Reduced the gap between industries and educational institutes. Research on new development in the IoT sector. Created research proposal for funding. Investigated the new communication protocol for IoT devices. Contact: Dr. Tan Chye Cheah - dr.chyecheah.t@apu.edu.my EDUCATION MSc in Technology Management Staffordshire University 11/2015 - 04/2017, Postgraduate with Distinction Challenges in Implementing IoT-enabled Smart cities in Malaysia. Bachelors Electrical Telecommunication Engineering COMSATS Institute of Information Technology, Islamabad 08/2010 - 01/2014, CGPA: 3.09 Networking Satellite communications Programming/ Matlab Telecommunication Engineering SKILLS Designing Leadership Media/Marketing R/Python SQL Tableau NLP Data Analysis Machine learning Deep learning Webapp/Cloud Feature Engineering Ensembling Time Series Technology Management ACHIEVEMENTS 98th Hungry Geese Simulation Competition (08/2021) 2nd in Covid-19 vaccinations around the world (07/2021) 8th in Automatic Speech Recognition in WOLOF (06/2021) Top 10 in WiDS Datathon. (03/2021) 40th / 622 in MagNet: Model the Geomagnetic Field Hosted by NOAA (02/2021) 18th in Rock, Paper, Scissors/Designing AI Agent Competition. (02/2021) PROJECTS Goodreads Profile Analysis WebApp (09/2021) Data Analysis Web Scraping XLM Interactive Visualization Contributed in orchest.io (08/2021) Testing and Debuging Technical Article Proposing new was to Improve ML pipelines World Vaccine Update System (06/2021) Used sqlite3 for database Automated system for daily update the Kaggle DB and Analysis Interactive dashboard mRNA-Vaccine-Degradation-Prediction (06/2021) Explore our dataset and then preprocessed sequence, structure, and predicted loop type features Train deep learning GRU model Trip Advisor Data Analysis/ML (04/2021) Preprocessing Data, Exploratory Data analysis, Word clouds. Feature Engineering, Text processing. BiLSTM Model for predicting rating, evaluation, model performance. Jane Street Market Prediction (03/2021) EDA, Feature Engineering, experimenting with hyperparameters. Ensembling: Resnet, NN Embeddings, TF Simple NN model. Using simple MLP pytorch model. Achievements/Tasks Achievements/Tasks Achievements/Tasks Thesis Courses\"\n",
    "# Print recognized entities and their labels\n",
    "print_entities(newnlp, test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/RanjithJames/.pyenv/versions/3.10.14/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'Repository' (from 'huggingface_hub.repository') is deprecated and will be removed from version '1.0'. Please prefer the http-based alternatives instead. Given its large adoption in legacy code, the complete removal is only planned on next major release.\n",
      "For more details, please read https://huggingface.co/docs/huggingface_hub/concepts/git_vs_http.\n",
      "  warnings.warn(warning_message, FutureWarning)\n",
      "Cloning https://huggingface.co/Ranjithjames/SPACY_NER into local empty directory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /Users/RanjithJames/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding files tracked by Git LFS: ['vocab/strings.json', 'vocab/vectors', '.DS_Store', 'attribute_ruler/patterns', 'ner/model', 'ner/moves', 'parser/model', 'parser/moves', 'senter/model', 'tagger/model', 'tok2vec/model', 'tokenizer', 'vocab/key2row']. This may take a bit of time if the files are large.\n",
      "Upload file vocab/vectors:   0%|          | 1.00/588M [00:00<?, ?B/s]\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Upload file vocab/vectors:   8%|▊         | 45.8M/588M [00:18<03:39, 2.59MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Upload file vocab/vectors: 595MB [03:25, 3.56MB/s]                            To https://huggingface.co/Ranjithjames/SPACY_NER\n",
      "   749863e..d2a5382  main -> main\n",
      "\n",
      "Upload file vocab/vectors: 100%|██████████| 588M/588M [03:26<00:00, 2.98MB/s]\n",
      "Upload file vocab/strings.json: 100%|██████████| 10.2M/10.2M [03:26<00:00, 51.6kB/s]\n",
      "\n",
      "Upload file tok2vec/model: 100%|██████████| 6.19M/6.19M [03:26<00:00, 31.4kB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "Upload file senter/model: 100%|██████████| 215k/215k [03:26<00:00, 1.06kB/s]\n",
      "\n",
      "\n",
      "\n",
      "Upload file vocab/key2row: 100%|██████████| 6.74M/6.74M [03:26<00:00, 34.2kB/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Upload file ner/model: 100%|██████████| 6.09M/6.09M [03:26<00:00, 30.9kB/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Upload file lemmatizer/lookups/lookups.bin: 100%|██████████| 950k/950k [03:26<00:00, 4.70kB/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Upload file parser/model: 100%|██████████| 312k/312k [03:26<00:00, 1.55kB/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Upload file tokenizer: 100%|██████████| 75.3k/75.3k [03:26<00:00, 373B/s]  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Upload file vocab/lookups.bin: 100%|██████████| 68.4k/68.4k [03:26<00:00, 339B/s]  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Upload file tagger/model: 100%|██████████| 19.4k/19.4k [03:26<00:00, 95.9B/s] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Upload file attribute_ruler/patterns: 100%|██████████| 14.4k/14.4k [03:26<00:00, 71.2B/s] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Upload file .DS_Store: 100%|██████████| 8.00k/8.00k [03:25<00:00, 39.8B/s] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Upload file parser/moves: 100%|██████████| 1.53k/1.53k [03:25<00:00, 7.63B/s] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Upload file ner/moves: 100%|██████████| 1.08k/1.08k [03:25<00:00, 5.38B/s] \n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "from huggingface_hub import Repository, login\n",
    "\n",
    "def save_model_to_huggingface(nlp, model_dir, repo_id, token):\n",
    "    \"\"\"\n",
    "    Save the trained spaCy model to Hugging Face Model Hub by cloning to a new folder and moving files.\n",
    "    \n",
    "    Args:\n",
    "        nlp (Language): The spaCy model to be saved.\n",
    "        model_dir (str): The directory path where the model will be saved.\n",
    "        repo_id (str): The repository ID on Hugging Face (e.g., 'Ranjithjames/SPACY_NER').\n",
    "        token (str): Your Hugging Face API token.\n",
    "    \"\"\"\n",
    "    # Save the spaCy model to the specified directory\n",
    "    nlp.to_disk(model_dir)\n",
    "\n",
    "    # Authenticate the Hugging Face Hub\n",
    "    login(token=token)\n",
    "    \n",
    "    # Temporary directory for cloning the repository\n",
    "    temp_model_dir = model_dir + \"_temp\"\n",
    "    \n",
    "    # Clone the repository to the temporary directory\n",
    "    repo_url = f\"https://huggingface.co/{repo_id}\"\n",
    "    repo = Repository(local_dir=temp_model_dir, clone_from=repo_url, use_auth_token=token)\n",
    "    \n",
    "    # Move the saved model files to the cloned repository directory\n",
    "    for item in os.listdir(model_dir):\n",
    "        s = os.path.join(model_dir, item)\n",
    "        d = os.path.join(temp_model_dir, item)\n",
    "        if os.path.isdir(s):\n",
    "            shutil.move(s, d)\n",
    "        else:\n",
    "            shutil.move(s, d)\n",
    "    \n",
    "    # Add and push the model files to the repository\n",
    "    repo.git_add(auto_lfs_track=True)\n",
    "    repo.git_commit(\"Update spaCy model\")\n",
    "    repo.git_push()\n",
    "    \n",
    "    # Clean up: move files back to original directory and remove temporary directory\n",
    "    for item in os.listdir(temp_model_dir):\n",
    "        s = os.path.join(temp_model_dir, item)\n",
    "        d = os.path.join(model_dir, item)\n",
    "        if os.path.isdir(s):\n",
    "            shutil.move(s, d)\n",
    "        else:\n",
    "            shutil.move(s, d)\n",
    "    shutil.rmtree(temp_model_dir)\n",
    "\n",
    "# Example usage\n",
    "model_dir = \"data/model\"\n",
    "repo_id = \"Ranjithjames/SPACY_NER\"\n",
    "token = \"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\"\n",
    "\n",
    "save_model_to_huggingface(newnlp, model_dir, repo_id, token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "def download_model_from_huggingface(repo_id, model_dir, token):\n",
    "    \"\"\"\n",
    "    Download the model from Hugging Face Model Hub and store it in the specified directory.\n",
    "    \n",
    "    Args:\n",
    "        repo_id (str): The repository ID on Hugging Face (e.g., 'Ranjithjames/SPACY_NER').\n",
    "        model_dir (str): The directory path where the model will be stored.\n",
    "        token (str): Your Hugging Face API token.\n",
    "    \"\"\"\n",
    "    # Create the model directory if it doesn't exist\n",
    "    if not os.path.exists(model_dir):\n",
    "        os.makedirs(model_dir)\n",
    "    \n",
    "    # Download the model files from the Hugging Face repository\n",
    "    files_to_download = [\"config.json\", \"tokenizer.json\", \"model.bin\", \"special_tokens_map.json\", \"vocab.txt\"]\n",
    "    \n",
    "    for file_name in files_to_download:\n",
    "        file_path = hf_hub_download(repo_id=repo_id, filename=file_name, use_auth_token=token)\n",
    "        shutil.move(file_path, os.path.join(model_dir, file_name))\n",
    "        \n",
    "# Example usage\n",
    "model_dir = \"data/model\"\n",
    "repo_id = \"Ranjithjames/SPACY_NER\"\n",
    "token = \"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\"\n",
    "\n",
    "download_model_from_huggingface(repo_id, model_dir, token)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
