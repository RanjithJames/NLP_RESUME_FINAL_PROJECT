{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Specify the path to your CSV file\n",
    "file_location = R\"data/Resume.csv\"\n",
    "resume_data = pd.read_csv(file_location)\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "resume_data.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import requests\n",
    "from io import StringIO\n",
    "\n",
    "# URL for the JSON Lines file\n",
    "json_url = 'https://raw.githubusercontent.com/RanjithJames/NLP_RESUME_FINAL_PROJECT/main/Skill_patterns.jsonl'\n",
    "\n",
    "# Retrieve the data from the URL\n",
    "response = requests.get(json_url)\n",
    "response.raise_for_status()  # Raises an exception for HTTP errors\n",
    "\n",
    "# Convert JSON Lines string into a pandas DataFrame\n",
    "skills_data = pd.read_json(StringIO(response.text), lines=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Download the spaCy model\n",
    "#!python -m spacy download en_core_web_lg\n",
    "\n",
    "#!python -m spacy download en_core_web_trf\n",
    "# !pip install spacy-transformers\n",
    "\n",
    "#!python -m spacy download en_core_web_md\n",
    "\n",
    "#!pip install spacy[cuda]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "# Enable GPU usage\n",
    "spacy.require_gpu()\n",
    "# Load the pre-trained spaCy model\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "# Initialize the matcher with the vocabulary\n",
    "skill_matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Add patterns to the matcher\n",
    "for idx, row in skills_data.iterrows():\n",
    "    skill_pattern = row['pattern']  # Assuming this is a list of dictionaries\n",
    "    skill_matcher.add(\"SKILL\", [skill_pattern])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install tqdm for progress tracking\n",
    "#!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def extract_skills_from_resumes(resume_texts):\n",
    "    extracted_data = []\n",
    "    for resume in tqdm(resume_texts, desc=\"Extracting skills\"):\n",
    "        doc = nlp(resume)\n",
    "        match_results = skill_matcher(doc)\n",
    "        skill_spans = [doc[start:end] for _, start, end in match_results]\n",
    "        entities = [(span.start_char, span.end_char, \"SKILL\") for span in skill_spans if span.text.strip()]\n",
    "        extracted_data.append((resume, {\"entities\": entities}))\n",
    "    return extracted_data\n",
    "\n",
    "# Limit the data to the first 5 resumes\n",
    "sample_resumes = resume_data['Resume_str'].tolist()\n",
    "\n",
    "# Extract skills from the sample resumes\n",
    "sample_training_data = extract_skills_from_resumes(sample_resumes)\n",
    "\n",
    "# Print the first item of the training data\n",
    "print(sample_training_data[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Save sample_training_data to a file\n",
    "# with open('sample_training_data.pkl', 'wb') as f:\n",
    "#     pickle.dump(sample_training_data, f)\n",
    "\n",
    "# Load sample_training_data from the file\n",
    "with open('sample_training_data.pkl', 'rb') as f:\n",
    "    sample_training_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Install scikit-learn for model training\n",
    "#!pip install scikit-learn\n",
    "#!pip install spacy-lookups-data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Unzip the training data into texts and annotations\n",
    "texts, annotations = zip(*sample_training_data)\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "train_texts, val_texts, train_annotations, val_annotations = train_test_split(\n",
    "    texts, annotations, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Recreate the training and validation sets as tuples (text, annotation)\n",
    "train_data = list(zip(train_texts, train_annotations))\n",
    "val_data = list(zip(val_texts, val_annotations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Add the directory to the PATH environment variable\n",
    "os.environ['PATH'] += os.pathsep + r'G:\\bin'\n",
    "\n",
    "# Verify the PATH has been updated\n",
    "print(os.environ['PATH'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install spacy[cuda]\n",
    "#!pip install cupy-cuda12x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from spacy.tokens import Span\n",
    "from spacy.training import Example\n",
    "from spacy.lookups import load_lookups\n",
    "from spacy.matcher import Matcher\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "\n",
    "\n",
    "ner = nlp.get_pipe(\"ner\")\n",
    "\n",
    "# Function to remove overlapping entities\n",
    "def remove_overlapping_entities(entities):\n",
    "    \"\"\"\n",
    "    Remove overlapping entities from the list.\n",
    "    \n",
    "    Args:\n",
    "        entities (list): List of tuples containing start, end, and label of entities.\n",
    "    \n",
    "    Returns:\n",
    "        list: List of non-overlapping entities.\n",
    "    \"\"\"\n",
    "    entities = sorted(entities, key=lambda x: x[0])  # Sort by start position\n",
    "    non_overlapping_entities = []\n",
    "    last_end = -1\n",
    "    for start, end, label in entities:\n",
    "        if start >= last_end:  # No overlap\n",
    "            non_overlapping_entities.append((start, end, label))\n",
    "            last_end = end\n",
    "    return non_overlapping_entities\n",
    "\n",
    "# Add the 'SKILL' entity label to the NER model if it's not already known\n",
    "if 'SKILL' not in ner.labels:\n",
    "    ner.add_label(\"SKILL\")\n",
    "\n",
    "# Function to add custom entity labels from training data\n",
    "def add_custom_entity_labels(train_data):\n",
    "    \"\"\"\n",
    "    Add custom entity labels from training data to the NER model.\n",
    "    \n",
    "    Args:\n",
    "        train_data (list): List of tuples containing text and annotations.\n",
    "    \"\"\"\n",
    "    for _, annotations in train_data:\n",
    "        for ent in annotations.get(\"entities\"):\n",
    "            if ent[2] not in ner.labels:\n",
    "                ner.add_label(ent[2])\n",
    "\n",
    "# Add custom entities from training data\n",
    "add_custom_entity_labels(train_data)\n",
    "\n",
    "# Disable other pipeline components during training to train only NER\n",
    "unaffected_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
    "\n",
    "# Function to preprocess training data to remove overlapping entities\n",
    "def preprocess_training_data(train_data):\n",
    "    \"\"\"\n",
    "    Preprocess the training data to remove overlapping entities.\n",
    "    \n",
    "    Args:\n",
    "        train_data (list): List of tuples containing text and annotations.\n",
    "    \n",
    "    Returns:\n",
    "        list: List of preprocessed training data.\n",
    "    \"\"\"\n",
    "    preprocessed_data = []\n",
    "    for text, annotations in train_data:\n",
    "        entities = annotations.get(\"entities\")\n",
    "        non_overlapping_entities = remove_overlapping_entities(entities)\n",
    "        preprocessed_data.append((text, {\"entities\": non_overlapping_entities}))\n",
    "    return preprocessed_data\n",
    "\n",
    "# Preprocess training data to remove overlapping entities\n",
    "train_data = preprocess_training_data(train_data)\n",
    "\n",
    "# Function to train NER model\n",
    "def train_ner_model(train_data, n_iter=1):\n",
    "    \"\"\"\n",
    "    Train the NER model with the given training data.\n",
    "    \n",
    "    Args:\n",
    "        train_data (list): List of tuples containing text and annotations.\n",
    "        n_iter (int): Number of training iterations.\n",
    "    \"\"\"\n",
    "    with nlp.disable_pipes(*unaffected_pipes):  # Only train NER\n",
    "        optimizer = nlp.initialize()  # Correct initialization for transformer-based models\n",
    "        for itn in range(n_iter):\n",
    "            random.shuffle(train_data)\n",
    "            losses = {}\n",
    "            for text, annotations in tqdm(train_data, desc=f\"Iteration {itn+1}\"):\n",
    "                example = Example.from_dict(nlp.make_doc(text), annotations)\n",
    "                nlp.update([example], sgd=optimizer, drop=0.5, losses=losses)\n",
    "            print(f\"Iteration {itn+1} Losses: {losses}\")\n",
    "\n",
    "# Train the NER model\n",
    "train_ner_model(train_data[1:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import spacy\n",
    "from spacy.tokens import Span\n",
    "from spacy.training import Example\n",
    "from spacy.scorer import Scorer\n",
    "\n",
    "# Define a valid path for saving the model\n",
    "model_dir = r\"data/model\"\n",
    "\n",
    "# Create the directory if it does not exist\n",
    "if not os.path.exists(model_dir):\n",
    "    os.makedirs(model_dir)\n",
    "\n",
    "# Save the trained model to disk\n",
    "def save_model(nlp, model_dir):\n",
    "    \"\"\"\n",
    "    Save the trained spaCy model to disk.\n",
    "    \n",
    "    Args:\n",
    "        nlp (Language): The spaCy model to be saved.\n",
    "        model_dir (str): The directory path where the model will be saved.\n",
    "    \"\"\"\n",
    "    nlp.to_disk(model_dir)\n",
    "\n",
    "save_model(nlp, model_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model from the saved directory into a new variable\n",
    "\n",
    "\n",
    "def load_model(model_dir):\n",
    "    \"\"\"\n",
    "    Load a spaCy model from a specified directory.\n",
    "    \n",
    "    Args:\n",
    "        model_dir (str): The directory path from where the model will be loaded.\n",
    "    \n",
    "    Returns:\n",
    "        Language: The loaded spaCy model.\n",
    "    \"\"\"\n",
    "    return spacy.load(model_dir)\n",
    "\n",
    "newnlp = load_model('data/model')\n",
    "\n",
    "\n",
    "# Process a new text through the loaded model\n",
    "def print_entities(ner_model, text):\n",
    "    \"\"\"\n",
    "    Process the text through the NER model and print the recognized entities and their labels.\n",
    "    \n",
    "    Args:\n",
    "        ner_model (Language): The spaCy NER model.\n",
    "        text (str): The text to be processed.\n",
    "    \"\"\"\n",
    "    doc = ner_model(text)\n",
    "    for ent in doc.ents:\n",
    "        print(ent.text, ent.label_)\n",
    "\n",
    "# Example text to process\n",
    "test_text = \"\"\"### Data Scientist Job Description\n",
    "\n",
    "**Job Title:** Data Scientist\n",
    "\n",
    "**Location:** [City, State]\n",
    "\n",
    "**Company:** [Company Name]\n",
    "\n",
    "**Job Type:** Full-time\n",
    "\n",
    "**About Us:**\n",
    "[Company Name] is a leading [industry] company, dedicated to [mission statement or company goal]. We leverage cutting-edge technologies and data-driven strategies to drive innovation and make informed decisions that impact our business and customers. We are seeking a passionate and analytical Data Scientist to join our dynamic team.\n",
    "\n",
    "**Job Summary:**\n",
    "As a Data Scientist at [Company Name], you will play a key role in driving data-driven decision-making by developing, deploying, and maintaining machine learning models and advanced analytics solutions. You will work closely with cross-functional teams to analyze large datasets, uncover insights, and solve complex business problems. This role requires a deep understanding of statistical methods, machine learning algorithms, and data manipulation techniques.\n",
    "\n",
    "**Key Responsibilities:**\n",
    "- **Data Analysis & Exploration:**\n",
    "  - Collect, process, and analyze large datasets from various sources to identify trends, patterns, and anomalies.\n",
    "  - Perform exploratory data analysis (EDA) to generate insights and support business decisions.\n",
    "  \n",
    "- **Model Development:**\n",
    "  - Develop, test, and deploy predictive models and machine learning algorithms to solve business problems.\n",
    "  - Optimize models for performance, scalability, and interpretability.\n",
    "  \n",
    "- **Collaboration & Communication:**\n",
    "  - Work closely with business stakeholders to understand their needs and translate them into analytical solutions.\n",
    "  - Communicate complex analytical concepts and insights to non-technical stakeholders in a clear and concise manner.\n",
    "\n",
    "- **Data Visualization:**\n",
    "  - Create interactive dashboards and visualizations to present data insights and trends.\n",
    "  - Provide recommendations based on data-driven insights to support decision-making.\n",
    "\n",
    "- **Continuous Improvement:**\n",
    "  - Stay up-to-date with the latest advancements in data science, machine learning, and AI technologies.\n",
    "  - Identify opportunities to improve data processes and contribute to the development of data-driven strategies.\n",
    "\n",
    "**Qualifications:**\n",
    "- Bachelor’s degree in Computer Science, Statistics, Mathematics, or a related field; Master’s or PhD preferred.\n",
    "- 2+ years of experience in data science, machine learning, or a related field.\n",
    "- Proficiency in programming languages such as Python, R, or SQL.\n",
    "- Experience with machine learning frameworks and libraries such as TensorFlow, scikit-learn, or PyTorch.\n",
    "- Strong knowledge of statistical methods, data mining, and predictive modeling techniques.\n",
    "- Experience with data visualization tools like Tableau, Power BI, or matplotlib.\n",
    "- Familiarity with big data tools and platforms such as Hadoop, Spark, or AWS.\n",
    "- Excellent problem-solving skills and attention to detail.\n",
    "- Strong communication skills and ability to work collaboratively in a team environment.\n",
    "\n",
    "**Preferred Skills:**\n",
    "- Experience with natural language processing (NLP) or deep learning.\n",
    "- Knowledge of cloud computing and data engineering.\n",
    "- Experience with A/B testing and experiment design.\n",
    "\n",
    "**Benefits:**\n",
    "- Competitive salary and performance-based bonuses.\n",
    "- Health, dental, and vision insurance.\n",
    "- 401(k) plan with company match.\n",
    "- Generous paid time off and holiday schedule.\n",
    "- Opportunities for professional development and career growth.\n",
    "\n",
    "**How to Apply:**\n",
    "Interested candidates should submit their resume, cover letter, and portfolio of relevant work to [email address] with the subject line “Data Scientist Application – [Your Name]”.\n",
    "\n",
    "[Company Name] is an equal opportunity employer. We celebrate diversity and are committed to creating an inclusive environment for all employees.\"\"\"\n",
    "# Print recognized entities and their labels\n",
    "print_entities(newnlp, test_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "from huggingface_hub import Repository, login\n",
    "\n",
    "def save_model_to_huggingface(nlp, model_dir, repo_id, token):\n",
    "    \"\"\"\n",
    "    Save the trained spaCy model to Hugging Face Model Hub by cloning to a new folder and moving files.\n",
    "    \n",
    "    Args:\n",
    "        nlp (Language): The spaCy model to be saved.\n",
    "        model_dir (str): The directory path where the model will be saved.\n",
    "        repo_id (str): The repository ID on Hugging Face (e.g., 'Ranjithjames/SPACY_NER').\n",
    "        token (str): Your Hugging Face API token.\n",
    "    \"\"\"\n",
    "    # Save the spaCy model to the specified directory\n",
    "    nlp.to_disk(model_dir)\n",
    "\n",
    "    # Authenticate the Hugging Face Hub\n",
    "    login(token=token)\n",
    "    \n",
    "    # Temporary directory for cloning the repository\n",
    "    temp_model_dir = model_dir + \"_temp\"\n",
    "    \n",
    "    # Clone the repository to the temporary directory\n",
    "    repo_url = f\"https://huggingface.co/{repo_id}\"\n",
    "    repo = Repository(local_dir=temp_model_dir, clone_from=repo_url, use_auth_token=token)\n",
    "    \n",
    "    # Move the saved model files to the cloned repository directory\n",
    "    for item in os.listdir(model_dir):\n",
    "        s = os.path.join(model_dir, item)\n",
    "        d = os.path.join(temp_model_dir, item)\n",
    "        if os.path.isdir(s):\n",
    "            shutil.move(s, d)\n",
    "        else:\n",
    "            shutil.move(s, d)\n",
    "    \n",
    "    # Add and push the model files to the repository\n",
    "    repo.git_add(auto_lfs_track=True)\n",
    "    repo.git_commit(\"Update spaCy model\")\n",
    "    repo.git_push()\n",
    "    \n",
    "    # Clean up: move files back to original directory and remove temporary directory\n",
    "    for item in os.listdir(temp_model_dir):\n",
    "        s = os.path.join(temp_model_dir, item)\n",
    "        d = os.path.join(model_dir, item)\n",
    "        if os.path.isdir(s):\n",
    "            shutil.move(s, d)\n",
    "        else:\n",
    "            shutil.move(s, d)\n",
    "    shutil.rmtree(temp_model_dir)\n",
    "\n",
    "# Example usage\n",
    "model_dir = \"data/model\"\n",
    "repo_id = \"Ranjithjames/SPACY_NER\"\n",
    "token = \"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\"\n",
    "\n",
    "save_model_to_huggingface(newnlp, model_dir, repo_id, token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi, hf_hub_download\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "def download_all_files_from_huggingface(repo_id, model_dir, token):\n",
    "    \"\"\"\n",
    "    Download all files from the Hugging Face Model Hub repository and store them in the specified directory.\n",
    "    \n",
    "    Args:\n",
    "        repo_id (str): The repository ID on Hugging Face (e.g., 'Ranjithjames/SPACY_NER').\n",
    "        model_dir (str): The directory path where the model will be stored.\n",
    "        token (str): Your Hugging Face API token.\n",
    "    \"\"\"\n",
    "    # Create the model directory if it doesn't exist\n",
    "    if not os.path.exists(model_dir):\n",
    "        os.makedirs(model_dir)\n",
    "    \n",
    "    # Initialize the Hugging Face API\n",
    "    api = HfApi()\n",
    "    \n",
    "    # List all files in the repository\n",
    "    repo_files = api.list_repo_files(repo_id=repo_id, use_auth_token=token)\n",
    "    \n",
    "    # Download each file and move it to the model directory\n",
    "    for file_name in repo_files:\n",
    "        # Download the file from the repository\n",
    "        file_path = hf_hub_download(repo_id=repo_id, filename=file_name, use_auth_token=token)\n",
    "        \n",
    "        # Determine the destination path\n",
    "        destination_path = os.path.join(model_dir, file_name)\n",
    "        \n",
    "        # Ensure the directory exists\n",
    "        os.makedirs(os.path.dirname(destination_path), exist_ok=True)\n",
    "        \n",
    "        # Move the file to the destination directory\n",
    "        shutil.move(file_path, destination_path)\n",
    "\n",
    "# Example usage\n",
    "model_dir = \"data/modell\"\n",
    "repo_id = \"Ranjithjames/SPACY_NER\"\n",
    "token = \"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\"\n",
    "\n",
    "download_all_files_from_huggingface(repo_id, model_dir, token)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix, classification_report\n",
    "from tqdm import tqdm\n",
    "from spacy.training import offsets_to_biluo_tags, biluo_tags_to_offsets\n",
    "import numpy as np\n",
    "\n",
    "# Load the model\n",
    "def load_model(model_dir):\n",
    "    \"\"\"\n",
    "    Load a spaCy model from a specified directory.\n",
    "    \n",
    "    Args:\n",
    "        model_dir (str): The directory path from where the model will be loaded.\n",
    "    \n",
    "    Returns:\n",
    "        Language: The loaded spaCy model.\n",
    "    \"\"\"\n",
    "    return spacy.load(model_dir)\n",
    "\n",
    "newnlp = load_model('data/model')\n",
    "\n",
    "def non_overlapping_entities(entities):\n",
    "    \"\"\"\n",
    "    Ensure entities are non-overlapping by sorting and filtering.\n",
    "    \n",
    "    Args:\n",
    "        entities (list of tuples): List of entities (start, end, label).\n",
    "    \n",
    "    Returns:\n",
    "        list of tuples: Filtered non-overlapping entities.\n",
    "    \"\"\"\n",
    "    sorted_entities = sorted(entities, key=lambda x: x[0])\n",
    "    non_overlapping = []\n",
    "\n",
    "    for ent in sorted_entities:\n",
    "        if not non_overlapping or ent[0] >= non_overlapping[-1][1]:\n",
    "            non_overlapping.append(ent)\n",
    "    return non_overlapping\n",
    "\n",
    "# Function to evaluate the model with progress tracking\n",
    "def evaluate_model(ner_model, test_data):\n",
    "    \"\"\"\n",
    "    Evaluate the NER model using the test data and calculate precision, recall, F1 scores, and support.\n",
    "    \n",
    "    Args:\n",
    "        ner_model (Language): The spaCy NER model.\n",
    "        test_data (list of tuples): A list where each tuple is of the form (text, {'entities': [(start, end, label), ...]})\n",
    "    \n",
    "    Returns:\n",
    "        dict: A dictionary containing precision, recall, F1 scores, support, and confusion matrix.\n",
    "    \"\"\"\n",
    "    true_entities = []\n",
    "    pred_entities = []\n",
    "\n",
    "    for text, annotations in tqdm(test_data, desc=\"Evaluating\"):\n",
    "        doc = ner_model(text)\n",
    "        true_ents = non_overlapping_entities(annotations['entities'])\n",
    "        pred_ents = [(ent.start_char, ent.end_char, ent.label_) for ent in doc.ents]\n",
    "        \n",
    "        # Generate token-based BIO tags for the true entities\n",
    "        true_tags = offsets_to_biluo_tags(ner_model.make_doc(text), true_ents)\n",
    "        \n",
    "        # Generate token-based BIO tags for the predicted entities\n",
    "        pred_tags = offsets_to_biluo_tags(ner_model.make_doc(text), pred_ents)\n",
    "        \n",
    "        # Filter to only include entities with 'skill' in their label (case insensitive)\n",
    "        filtered_true_tags = [tag for tag in true_tags if 'skill' in tag.lower() and tag.lower() != 'l-skill']\n",
    "        filtered_pred_tags = [tag for tag in pred_tags if 'skill' in tag.lower() and tag.lower() != 'l-skill']\n",
    "\n",
    "\n",
    "        # Trim both lists to the same length\n",
    "        min_length = min(len(filtered_true_tags), len(filtered_pred_tags))\n",
    "        filtered_true_tags = filtered_true_tags[:min_length]\n",
    "        filtered_pred_tags = filtered_pred_tags[:min_length]\n",
    "        \n",
    "        # print(filtered_true_tags)\n",
    "        # print(filtered_pred_tags)\n",
    "        # Append to the lists\n",
    "        true_entities.extend(filtered_true_tags)\n",
    "        pred_entities.extend(filtered_pred_tags)\n",
    "        \n",
    "\n",
    "    # Calculate precision, recall, f1, and support\n",
    "    precision, recall, f1, support = precision_recall_fscore_support(true_entities, pred_entities, average='weighted')\n",
    "\n",
    "    # Generate confusion matrix\n",
    "    labels = list(set(true_entities + pred_entities))\n",
    "    conf_matrix = confusion_matrix(true_entities, pred_entities, labels=labels)\n",
    "    \n",
    "    # Generate classification report\n",
    "    class_report = classification_report(true_entities, pred_entities, labels=labels)\n",
    "\n",
    "    return {\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'support': support,\n",
    "        'confusion_matrix': conf_matrix,\n",
    "        'classification_report': class_report\n",
    "    }\n",
    "\n",
    "\n",
    "# Evaluate the model with progress tracking\n",
    "evaluation_results = evaluate_model(newnlp, val_data)\n",
    "\n",
    "print(\"Precision: {:.4f}\".format(evaluation_results['precision']))\n",
    "print(\"Recall: {:.4f}\".format(evaluation_results['recall']))\n",
    "print(\"F1 Score: {:.4f}\".format(evaluation_results['f1']))\n",
    "print(\"Confusion Matrix:\\n{}\".format(evaluation_results['confusion_matrix']))\n",
    "print(\"Classification Report:\\n{}\".format(evaluation_results['classification_report']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
