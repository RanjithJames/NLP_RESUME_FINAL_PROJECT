{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Resume_str</th>\n",
       "      <th>Resume_html</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16852973</td>\n",
       "      <td>HR ADMINISTRATOR/MARKETING ASSOCIATE\\...</td>\n",
       "      <td>&lt;div class=\"fontsize fontface vmargins hmargin...</td>\n",
       "      <td>HR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>22323967</td>\n",
       "      <td>HR SPECIALIST, US HR OPERATIONS      ...</td>\n",
       "      <td>&lt;div class=\"fontsize fontface vmargins hmargin...</td>\n",
       "      <td>HR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>33176873</td>\n",
       "      <td>HR DIRECTOR       Summary      Over 2...</td>\n",
       "      <td>&lt;div class=\"fontsize fontface vmargins hmargin...</td>\n",
       "      <td>HR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27018550</td>\n",
       "      <td>HR SPECIALIST       Summary    Dedica...</td>\n",
       "      <td>&lt;div class=\"fontsize fontface vmargins hmargin...</td>\n",
       "      <td>HR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17812897</td>\n",
       "      <td>HR MANAGER         Skill Highlights  ...</td>\n",
       "      <td>&lt;div class=\"fontsize fontface vmargins hmargin...</td>\n",
       "      <td>HR</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ID                                         Resume_str  \\\n",
       "0  16852973           HR ADMINISTRATOR/MARKETING ASSOCIATE\\...   \n",
       "1  22323967           HR SPECIALIST, US HR OPERATIONS      ...   \n",
       "2  33176873           HR DIRECTOR       Summary      Over 2...   \n",
       "3  27018550           HR SPECIALIST       Summary    Dedica...   \n",
       "4  17812897           HR MANAGER         Skill Highlights  ...   \n",
       "\n",
       "                                         Resume_html Category  \n",
       "0  <div class=\"fontsize fontface vmargins hmargin...       HR  \n",
       "1  <div class=\"fontsize fontface vmargins hmargin...       HR  \n",
       "2  <div class=\"fontsize fontface vmargins hmargin...       HR  \n",
       "3  <div class=\"fontsize fontface vmargins hmargin...       HR  \n",
       "4  <div class=\"fontsize fontface vmargins hmargin...       HR  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Specify the path to your CSV file\n",
    "file_location = R\"data/Resume.csv\"\n",
    "resume_data = pd.read_csv(file_location)\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "resume_data.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import requests\n",
    "from io import StringIO\n",
    "\n",
    "# URL for the JSON Lines file\n",
    "json_url = 'https://raw.githubusercontent.com/mystery2life/NLP-Project/main/temp_patterns.jsonl'\n",
    "\n",
    "# Retrieve the data from the URL\n",
    "response = requests.get(json_url)\n",
    "response.raise_for_status()  # Raises an exception for HTTP errors\n",
    "\n",
    "# Convert JSON Lines string into a pandas DataFrame\n",
    "skills_data = pd.read_json(StringIO(response.text), lines=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Download the spaCy model\n",
    "#!python -m spacy download en_core_web_lg\n",
    "\n",
    "#!python -m spacy download en_core_web_trf\n",
    "# !pip install spacy-transformers\n",
    "\n",
    "# !python -m spacy download en_core_web_md\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the pre-trained spaCy model\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "# Initialize the matcher with the vocabulary\n",
    "skill_matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Add patterns to the matcher\n",
    "for idx, row in skills_data.iterrows():\n",
    "    skill_pattern = row['pattern']  # Assuming this is a list of dictionaries\n",
    "    skill_matcher.add(\"SKILL\", [skill_pattern])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in /Users/RanjithJames/.pyenv/versions/3.10.14/lib/python3.10/site-packages (4.66.4)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Install tqdm for progress tracking\n",
    "!pip install tqdm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def extract_skills_from_resumes(resume_texts):\n",
    "    extracted_data = []\n",
    "    for resume in tqdm(resume_texts, desc=\"Extracting skills\"):\n",
    "        doc = nlp(resume)\n",
    "        match_results = skill_matcher(doc)\n",
    "        skill_spans = [doc[start:end] for _, start, end in match_results]\n",
    "        entities = [(span.start_char, span.end_char, \"SKILL\") for span in skill_spans if span.text.strip()]\n",
    "        extracted_data.append((resume, {\"entities\": entities}))\n",
    "    return extracted_data\n",
    "\n",
    "# Limit the data to the first 5 resumes\n",
    "sample_resumes = resume_data['Resume_str'].tolist()\n",
    "\n",
    "# Extract skills from the sample resumes\n",
    "sample_training_data = extract_skills_from_resumes(sample_resumes)\n",
    "\n",
    "# Print the first item of the training data\n",
    "print(sample_training_data[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"         HR SPECIALIST, US HR OPERATIONS       Summary     Versatile  media professional with background in Communications, Marketing, Human Resources and Technology.\\xa0        Experience     09/2015   to   Current     HR Specialist, US HR Operations    Company Name   －   City  ,   State       Managed communication regarding launch of Operations group, policy changes and system outages      Designed standard work and job aids to create comprehensive training program for new employees and contractors         Audited job postings for old, pending, on-hold and draft positions.           Audited union hourly, non-union hourly and salary background checks and drug screens             Conducted monthly new hire benefits briefing to new employees across all business units               Served as a link between HR Managers and vendors by handling questions and resolving system-related issues         Provide real-time process improvement feedback on key metrics and initiatives  Successfully re-branded US HR Operations SharePoint site  Business Unit project manager for RFI/RFP on Background Check and Drug Screen vendor         01/2014   to   05/2015     IT, Marketing and Communications Co-op    Company Name   －   City  ,   State      Posted new articles, changes and updates to corporate SharePoint site including graphics and visual communications.  Researched and drafted articles and feature stories to promote company activities and programs.  Co-edited and developed content for quarterly published newsletter.  Provided communication support for internal and external events.  Collaborated with Communication team, media professionals and vendors to determine program needs for print materials, web design and digital communications.  Entrusted to lead product, service and software launches for Digital Asset Management tool, Marketing Toolkit website and Executive Tradeshows Calendar.  Created presentations for management and executive approval to ensure alignment with corporate guidelines and branding.  Maintained the MySikorsky SharePoint site and provided timely solutions to mitigate issues.\\xa0\\xa0\\xa0\\xa0  Created story board and produced video for annual IT All Hands meeting.         10/2012   to   01/2014     Relationship Coordinator/Marketing Specialist    Company Name   －   City  ,   State       Partnered with vendor to manage the in-house advertising program consisting of print and media collateral pieces.     Coordinated pre-show and post-show activities at trade shows.     Managed marketing campaigns to generate new business and to support partner and sales teams.     Ordered marketing collateral for meetings, trade shows and advisors.    Improved, administered and modified marketing programs to increase product awareness.  Assisted in preparing internal promotional publications, managed marketing material inventory and supervised distribution of publications to ensure high quality product output.  Coordinated marketing materials including brochures, promotional materials and products.  Partnered with graphic designers to develop appropriate materials and branding for brochures.  Used tracking and reporting systems for sales leads and appointments.         09/2009   to   10/2012     Assistant Head Teller    Company Name   －   City  ,   State       Received an internal audit score of  100 %.     Performed daily and monthly audits of ATM machines and tellers.     Educated customers on a variety of retail products and available credit options.       Consistently met or exceeded quarterly sales goals     Promoted products and services to\\ncustomers while maintaining company brand identity\\n\\n·\\xa0\\xa0\\xa0\\xa0\\n  Implemented programs to achieve\\nand exceed customer and company participation goals\\xa0\\n\\n\\xa0  Organized company sponsored events on campus resulting in increased\\nbrand awareness\\n\\n·\\xa0\\xa0\\xa0\\xa0\\n  Coached peers on\\nthe proper use of programs to improve work flow efficiency  Utilized product knowledge to successfully sell\\nto and refer clients based on individual needs  Promoted marketing the grand opening\\nof new branch locations to strengthen company brand affinity\\n\\n·\\xa0\\xa0\\xa0\\xa0   Organized company sponsored events\\nresulting in increased brand awareness and improved sales\\n\\n·\\xa0\\xa0\\xa0\\xa0   Coached peers on the proper use of\\nprograms to increase work flow efficiency\\n\\n          Senior Producer - 2014 SHU Media Exchange    Company Name   －   City  ,   State      Planned and executed event\\xa0focusing on Connecticut's creative corridor, growth of industry and opportunities that come with development. A\\xa0 panel of industry professionals addressed topics related to media and hosted a question and answer session for approximately 110 attendees. Following the forum, guests were invited to engage in networking and conversation at a post-event reception.         Education     2014     Master of Arts  :   Corporate Communication & Public Relations    Sacred Heart University   －   City  ,   State             2013     Bachelor of Arts  :   Relational Communication    Western Connecticut State University   －   City  ,   State              Skills    Adobe Photoshop, ADP, Asset Management, branding, brochures, content, Customer Care, Final Cut Pro, graphics, graphic, HR, Illustrator, InDesign, Innovation, inventory, Lotus Notes, marketing, marketing materials, marketing material, materials, Microsoft Office, SharePoint, newsletter, presentations, process improvement, Project Management, promotional materials, publications, Quality, real-time, Recruitment, reporting, RFP, sales, stories, Employee Development, video, web design, website, articles   \", {'entities': [(108, 122, 'SKILL'), (124, 133, 'SKILL'), (759, 767, 'SKILL'), (1040, 1048, 'SKILL'), (1164, 1173, 'SKILL'), (1178, 1192, 'SKILL'), (1342, 1356, 'SKILL'), (1548, 1555, 'SKILL'), (1713, 1719, 'SKILL'), (1732, 1746, 'SKILL'), (1788, 1796, 'SKILL'), (1841, 1850, 'SKILL'), (2253, 2262, 'SKILL'), (2363, 2374, 'SKILL'), (2510, 2519, 'SKILL'), (2546, 2554, 'SKILL'), (2562, 2569, 'SKILL'), (2607, 2616, 'SKILL'), (2707, 2716, 'SKILL'), (2823, 2832, 'SKILL'), (2833, 2841, 'SKILL'), (2948, 2957, 'SKILL'), (4008, 4017, 'SKILL'), (5066, 5081, 'SKILL'), (5248, 5257, 'SKILL'), (5259, 5268, 'SKILL'), (5280, 5289, 'SKILL'), (5290, 5298, 'SKILL'), (5389, 5407, 'SKILL'), (5544, 5550, 'SKILL')]})\n"
     ]
    }
   ],
   "source": [
    "print(sample_training_data[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Save sample_training_data to a file\n",
    "# with open('sample_training_data.pkl', 'wb') as f:\n",
    "#     pickle.dump(sample_training_data, f)\n",
    "\n",
    "# Load sample_training_data from the file\n",
    "with open('sample_training_data.pkl', 'rb') as f:\n",
    "    sample_training_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Install scikit-learn for model training\n",
    "#!pip install scikit-learn\n",
    "!pip install spacy-lookups-data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Unzip the training data into texts and annotations\n",
    "texts, annotations = zip(*sample_training_data)\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "train_texts, val_texts, train_annotations, val_annotations = train_test_split(\n",
    "    texts, annotations, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "# Recreate the training and validation sets as tuples (text, annotation)\n",
    "train_data = list(zip(train_texts, train_annotations))\n",
    "val_data = list(zip(val_texts, val_annotations))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nlp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 14\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpickle\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m ner \u001b[38;5;241m=\u001b[39m \u001b[43mnlp\u001b[49m\u001b[38;5;241m.\u001b[39mget_pipe(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mner\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Function to remove overlapping entities\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mremove_overlapping_entities\u001b[39m(entities):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nlp' is not defined"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from spacy.tokens import Span\n",
    "from spacy.training import Example\n",
    "from spacy.lookups import load_lookups\n",
    "from spacy.matcher import Matcher\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "\n",
    "\n",
    "ner = nlp.get_pipe(\"ner\")\n",
    "\n",
    "# Function to remove overlapping entities\n",
    "def remove_overlapping_entities(entities):\n",
    "    \"\"\"\n",
    "    Remove overlapping entities from the list.\n",
    "    \n",
    "    Args:\n",
    "        entities (list): List of tuples containing start, end, and label of entities.\n",
    "    \n",
    "    Returns:\n",
    "        list: List of non-overlapping entities.\n",
    "    \"\"\"\n",
    "    entities = sorted(entities, key=lambda x: x[0])  # Sort by start position\n",
    "    non_overlapping_entities = []\n",
    "    last_end = -1\n",
    "    for start, end, label in entities:\n",
    "        if start >= last_end:  # No overlap\n",
    "            non_overlapping_entities.append((start, end, label))\n",
    "            last_end = end\n",
    "    return non_overlapping_entities\n",
    "\n",
    "# Add the 'SKILL' entity label to the NER model if it's not already known\n",
    "if 'SKILL' not in ner.labels:\n",
    "    ner.add_label(\"SKILL\")\n",
    "\n",
    "# Function to add custom entity labels from training data\n",
    "def add_custom_entity_labels(train_data):\n",
    "    \"\"\"\n",
    "    Add custom entity labels from training data to the NER model.\n",
    "    \n",
    "    Args:\n",
    "        train_data (list): List of tuples containing text and annotations.\n",
    "    \"\"\"\n",
    "    for _, annotations in train_data:\n",
    "        for ent in annotations.get(\"entities\"):\n",
    "            if ent[2] not in ner.labels:\n",
    "                ner.add_label(ent[2])\n",
    "\n",
    "# Add custom entities from training data\n",
    "add_custom_entity_labels(train_data)\n",
    "\n",
    "# Disable other pipeline components during training to train only NER\n",
    "unaffected_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
    "\n",
    "# Function to preprocess training data to remove overlapping entities\n",
    "def preprocess_training_data(train_data):\n",
    "    \"\"\"\n",
    "    Preprocess the training data to remove overlapping entities.\n",
    "    \n",
    "    Args:\n",
    "        train_data (list): List of tuples containing text and annotations.\n",
    "    \n",
    "    Returns:\n",
    "        list: List of preprocessed training data.\n",
    "    \"\"\"\n",
    "    preprocessed_data = []\n",
    "    for text, annotations in train_data:\n",
    "        entities = annotations.get(\"entities\")\n",
    "        non_overlapping_entities = remove_overlapping_entities(entities)\n",
    "        preprocessed_data.append((text, {\"entities\": non_overlapping_entities}))\n",
    "    return preprocessed_data\n",
    "\n",
    "# Preprocess training data to remove overlapping entities\n",
    "train_data = preprocess_training_data(train_data)\n",
    "\n",
    "# Function to train NER model\n",
    "def train_ner_model(train_data, n_iter=10):\n",
    "    \"\"\"\n",
    "    Train the NER model with the given training data.\n",
    "    \n",
    "    Args:\n",
    "        train_data (list): List of tuples containing text and annotations.\n",
    "        n_iter (int): Number of training iterations.\n",
    "    \"\"\"\n",
    "    with nlp.disable_pipes(*unaffected_pipes):  # Only train NER\n",
    "        optimizer = nlp.initialize()  # Correct initialization for transformer-based models\n",
    "        for itn in range(n_iter):\n",
    "            random.shuffle(train_data)\n",
    "            losses = {}\n",
    "            for text, annotations in tqdm(train_data, desc=f\"Iteration {itn+1}\"):\n",
    "                example = Example.from_dict(nlp.make_doc(text), annotations)\n",
    "                nlp.update([example], sgd=optimizer, drop=0.5, losses=losses)\n",
    "            print(f\"Iteration {itn+1} Losses: {losses}\")\n",
    "\n",
    "# Train the NER model\n",
    "train_ner_model(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nlp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 25\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;124;03m    Save the trained spaCy model to disk.\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;124;03m    \u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;124;03m        model_dir (str): The directory path where the model will be saved.\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m     23\u001b[0m     nlp\u001b[38;5;241m.\u001b[39mto_disk(model_dir)\n\u001b[0;32m---> 25\u001b[0m save_model(\u001b[43mnlp\u001b[49m, model_dir)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Load the model from the saved directory into a new variable\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_model\u001b[39m(model_dir):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nlp' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import spacy\n",
    "from spacy.tokens import Span\n",
    "from spacy.training import Example\n",
    "from spacy.scorer import Scorer\n",
    "\n",
    "# Define a valid path for saving the model\n",
    "model_dir = r\"/Users/RanjithJames/Downloads/NLP-Project-project/data/model\"\n",
    "\n",
    "# Create the directory if it does not exist\n",
    "if not os.path.exists(model_dir):\n",
    "    os.makedirs(model_dir)\n",
    "\n",
    "# Save the trained model to disk\n",
    "def save_model(nlp, model_dir):\n",
    "    \"\"\"\n",
    "    Save the trained spaCy model to disk.\n",
    "    \n",
    "    Args:\n",
    "        nlp (Language): The spaCy model to be saved.\n",
    "        model_dir (str): The directory path where the model will be saved.\n",
    "    \"\"\"\n",
    "    nlp.to_disk(model_dir)\n",
    "\n",
    "save_model(nlp, model_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/RanjithJames/.pyenv/versions/3.10.14/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI SKILL\n",
      "testing SKILL\n",
      "AI SKILL\n",
      "Testing SKILL\n",
      "data analysis SKILL\n",
      "machine learning SKILL\n",
      "Marketing SKILL\n",
      "Engineering SKILL\n",
      "communications SKILL\n",
      "Engineering SKILL\n",
      "Marketing SKILL\n",
      "R SKILL\n",
      "Python SKILL\n",
      "SQL SKILL\n",
      "Tableau SKILL\n",
      "NLP SKILL\n",
      "Data Analysis SKILL\n",
      "Engineering SKILL\n",
      "Time Series SKILL\n",
      "Simulation SKILL\n",
      "AI SKILL\n",
      "Data Analysis SKILL\n",
      "Visualization SKILL\n",
      "Testing SKILL\n",
      "ML SKILL\n",
      "database SKILL\n",
      "Data Analysis SKILL\n",
      "ML SKILL\n",
      "Data analysis SKILL\n",
      "Engineering SKILL\n",
      "Engineering SKILL\n"
     ]
    }
   ],
   "source": [
    "# Load the model from the saved directory into a new variable\n",
    "\n",
    "\n",
    "def load_model(model_dir):\n",
    "    \"\"\"\n",
    "    Load a spaCy model from a specified directory.\n",
    "    \n",
    "    Args:\n",
    "        model_dir (str): The directory path from where the model will be loaded.\n",
    "    \n",
    "    Returns:\n",
    "        Language: The loaded spaCy model.\n",
    "    \"\"\"\n",
    "    return spacy.load(model_dir)\n",
    "\n",
    "newnlp = load_model('data/model')\n",
    "\n",
    "\n",
    "# Process a new text through the loaded model\n",
    "def print_entities(ner_model, text):\n",
    "    \"\"\"\n",
    "    Process the text through the NER model and print the recognized entities and their labels.\n",
    "    \n",
    "    Args:\n",
    "        ner_model (Language): The spaCy NER model.\n",
    "        text (str): The text to be processed.\n",
    "    \"\"\"\n",
    "    doc = ner_model(text)\n",
    "    for ent in doc.ents:\n",
    "        print(ent.text, ent.label_)\n",
    "\n",
    "# Example text to process\n",
    "test_text = \"Abid Ali Awan Data Scientist I am a certified data scientist professional, who loves building machine learning models and blogs about the latest AI technologies. I am currently testing AI Products at PEC-PITC, which later gets approved for human trials. abidaliawan@tutamail.com +923456855126 Islamabad, Pakistan abidaliawan.me WORK EXPERIENCE Data Scientist Pakistan Innovation and Testing Center - PEC 04/2021 - Present, Islamabad, Pakistan Redesigned data of engineers that were mostly scattered and unavailable. Designed dashboard and data analysis report to help higher management make better decisions. Accessibility of key information has created a new culture of making data-driven decisions. Contact: Ali Raza Asif - darkslayerraza10@gmail.com Data Scientist Freelancing/Kaggle 11/2020 - Present, Islamabad, Pakistan Engineered a healthcare system. Used machine learning to detect some of the common decisions. The project has paved the way for others to use new techniques to get better results. Participated in Kaggle machine learning competitions. Learned new techniques to get a better score and finally got to 1 percent rank. Researcher / Event Organizer CREDIT 02/2017 - 07/2017, Kuala Lumpur, Malaysia Marketing for newly build research lab. Organized technical events and successfully invited the multiple company's CEO for talks. Reduced the gap between industries and educational institutes. Research on new development in the IoT sector. Created research proposal for funding. Investigated the new communication protocol for IoT devices. Contact: Dr. Tan Chye Cheah - dr.chyecheah.t@apu.edu.my EDUCATION MSc in Technology Management Staffordshire University 11/2015 - 04/2017, Postgraduate with Distinction Challenges in Implementing IoT-enabled Smart cities in Malaysia. Bachelors Electrical Telecommunication Engineering COMSATS Institute of Information Technology, Islamabad 08/2010 - 01/2014, CGPA: 3.09 Networking Satellite communications Programming/ Matlab Telecommunication Engineering SKILLS Designing Leadership Media/Marketing R/Python SQL Tableau NLP Data Analysis Machine learning Deep learning Webapp/Cloud Feature Engineering Ensembling Time Series Technology Management ACHIEVEMENTS 98th Hungry Geese Simulation Competition (08/2021) 2nd in Covid-19 vaccinations around the world (07/2021) 8th in Automatic Speech Recognition in WOLOF (06/2021) Top 10 in WiDS Datathon. (03/2021) 40th / 622 in MagNet: Model the Geomagnetic Field Hosted by NOAA (02/2021) 18th in Rock, Paper, Scissors/Designing AI Agent Competition. (02/2021) PROJECTS Goodreads Profile Analysis WebApp (09/2021) Data Analysis Web Scraping XLM Interactive Visualization Contributed in orchest.io (08/2021) Testing and Debuging Technical Article Proposing new was to Improve ML pipelines World Vaccine Update System (06/2021) Used sqlite3 for database Automated system for daily update the Kaggle DB and Analysis Interactive dashboard mRNA-Vaccine-Degradation-Prediction (06/2021) Explore our dataset and then preprocessed sequence, structure, and predicted loop type features Train deep learning GRU model Trip Advisor Data Analysis/ML (04/2021) Preprocessing Data, Exploratory Data analysis, Word clouds. Feature Engineering, Text processing. BiLSTM Model for predicting rating, evaluation, model performance. Jane Street Market Prediction (03/2021) EDA, Feature Engineering, experimenting with hyperparameters. Ensembling: Resnet, NN Embeddings, TF Simple NN model. Using simple MLP pytorch model. Achievements/Tasks Achievements/Tasks Achievements/Tasks Thesis Courses\"\n",
    "# Print recognized entities and their labels\n",
    "print_entities(newnlp, test_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI SKILL\n",
      "testing SKILL\n",
      "AI SKILL\n",
      "Testing SKILL\n",
      "data analysis SKILL\n",
      "machine learning SKILL\n",
      "Marketing SKILL\n",
      "Engineering SKILL\n",
      "communications SKILL\n",
      "Engineering SKILL\n",
      "Marketing SKILL\n",
      "R SKILL\n",
      "Python SKILL\n",
      "SQL SKILL\n",
      "Tableau SKILL\n",
      "NLP SKILL\n",
      "Data Analysis SKILL\n",
      "Engineering SKILL\n",
      "Time Series SKILL\n",
      "Simulation SKILL\n",
      "AI SKILL\n",
      "Data Analysis SKILL\n",
      "Visualization SKILL\n",
      "Testing SKILL\n",
      "ML SKILL\n",
      "database SKILL\n",
      "Data Analysis SKILL\n",
      "ML SKILL\n",
      "Data analysis SKILL\n",
      "Engineering SKILL\n",
      "Engineering SKILL\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import spacy\n",
    "from spacy.tokens import Span\n",
    "from spacy.training import Example\n",
    "from spacy.scorer import Scorer\n",
    "\n",
    "def load_model(model_dir):\n",
    "    \"\"\"\n",
    "    Load a spaCy model from a specified directory.\n",
    "    \n",
    "    Args:\n",
    "        model_dir (str): The directory path from where the model will be loaded.\n",
    "    \n",
    "    Returns:\n",
    "        Language: The loaded spaCy model.\n",
    "    \"\"\"\n",
    "    return spacy.load(model_dir)\n",
    "newnlp = load_model(model_dir)\n",
    "\n",
    "\n",
    "# Process a new text through the loaded model\n",
    "def print_entities(ner_model, text):\n",
    "    \"\"\"\n",
    "    Process the text through the NER model and print the recognized entities and their labels.\n",
    "    \n",
    "    Args:\n",
    "        ner_model (Language): The spaCy NER model.\n",
    "        text (str): The text to be processed.\n",
    "    \"\"\"\n",
    "    doc = ner_model(text)\n",
    "    for ent in doc.ents:\n",
    "        print(ent.text, ent.label_)\n",
    "\n",
    "# Example text to process\n",
    "test_text = \"Abid Ali Awan Data Scientist I am a certified data scientist professional, who loves building machine learning models and blogs about the latest AI technologies. I am currently testing AI Products at PEC-PITC, which later gets approved for human trials. abidaliawan@tutamail.com +923456855126 Islamabad, Pakistan abidaliawan.me WORK EXPERIENCE Data Scientist Pakistan Innovation and Testing Center - PEC 04/2021 - Present, Islamabad, Pakistan Redesigned data of engineers that were mostly scattered and unavailable. Designed dashboard and data analysis report to help higher management make better decisions. Accessibility of key information has created a new culture of making data-driven decisions. Contact: Ali Raza Asif - darkslayerraza10@gmail.com Data Scientist Freelancing/Kaggle 11/2020 - Present, Islamabad, Pakistan Engineered a healthcare system. Used machine learning to detect some of the common decisions. The project has paved the way for others to use new techniques to get better results. Participated in Kaggle machine learning competitions. Learned new techniques to get a better score and finally got to 1 percent rank. Researcher / Event Organizer CREDIT 02/2017 - 07/2017, Kuala Lumpur, Malaysia Marketing for newly build research lab. Organized technical events and successfully invited the multiple company's CEO for talks. Reduced the gap between industries and educational institutes. Research on new development in the IoT sector. Created research proposal for funding. Investigated the new communication protocol for IoT devices. Contact: Dr. Tan Chye Cheah - dr.chyecheah.t@apu.edu.my EDUCATION MSc in Technology Management Staffordshire University 11/2015 - 04/2017, Postgraduate with Distinction Challenges in Implementing IoT-enabled Smart cities in Malaysia. Bachelors Electrical Telecommunication Engineering COMSATS Institute of Information Technology, Islamabad 08/2010 - 01/2014, CGPA: 3.09 Networking Satellite communications Programming/ Matlab Telecommunication Engineering SKILLS Designing Leadership Media/Marketing R/Python SQL Tableau NLP Data Analysis Machine learning Deep learning Webapp/Cloud Feature Engineering Ensembling Time Series Technology Management ACHIEVEMENTS 98th Hungry Geese Simulation Competition (08/2021) 2nd in Covid-19 vaccinations around the world (07/2021) 8th in Automatic Speech Recognition in WOLOF (06/2021) Top 10 in WiDS Datathon. (03/2021) 40th / 622 in MagNet: Model the Geomagnetic Field Hosted by NOAA (02/2021) 18th in Rock, Paper, Scissors/Designing AI Agent Competition. (02/2021) PROJECTS Goodreads Profile Analysis WebApp (09/2021) Data Analysis Web Scraping XLM Interactive Visualization Contributed in orchest.io (08/2021) Testing and Debuging Technical Article Proposing new was to Improve ML pipelines World Vaccine Update System (06/2021) Used sqlite3 for database Automated system for daily update the Kaggle DB and Analysis Interactive dashboard mRNA-Vaccine-Degradation-Prediction (06/2021) Explore our dataset and then preprocessed sequence, structure, and predicted loop type features Train deep learning GRU model Trip Advisor Data Analysis/ML (04/2021) Preprocessing Data, Exploratory Data analysis, Word clouds. Feature Engineering, Text processing. BiLSTM Model for predicting rating, evaluation, model performance. Jane Street Market Prediction (03/2021) EDA, Feature Engineering, experimenting with hyperparameters. Ensembling: Resnet, NN Embeddings, TF Simple NN model. Using simple MLP pytorch model. Achievements/Tasks Achievements/Tasks Achievements/Tasks Thesis Courses\"\n",
    "# Print recognized entities and their labels\n",
    "print_entities(newnlp, test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/RanjithJames/.pyenv/versions/3.10.14/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'Repository' (from 'huggingface_hub.repository') is deprecated and will be removed from version '1.0'. Please prefer the http-based alternatives instead. Given its large adoption in legacy code, the complete removal is only planned on next major release.\n",
      "For more details, please read https://huggingface.co/docs/huggingface_hub/concepts/git_vs_http.\n",
      "  warnings.warn(warning_message, FutureWarning)\n",
      "Cloning https://huggingface.co/Ranjithjames/SPACY_NER into local empty directory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /Users/RanjithJames/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding files tracked by Git LFS: ['vocab/strings.json', 'vocab/vectors', '.DS_Store', 'attribute_ruler/patterns', 'ner/model', 'ner/moves', 'parser/model', 'parser/moves', 'senter/model', 'tagger/model', 'tok2vec/model', 'tokenizer', 'vocab/key2row']. This may take a bit of time if the files are large.\n",
      "Upload file vocab/vectors:   0%|          | 1.00/588M [00:00<?, ?B/s]\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Upload file vocab/vectors:   8%|▊         | 45.8M/588M [00:18<03:39, 2.59MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Upload file vocab/vectors: 595MB [03:25, 3.56MB/s]                            To https://huggingface.co/Ranjithjames/SPACY_NER\n",
      "   749863e..d2a5382  main -> main\n",
      "\n",
      "Upload file vocab/vectors: 100%|██████████| 588M/588M [03:26<00:00, 2.98MB/s]\n",
      "Upload file vocab/strings.json: 100%|██████████| 10.2M/10.2M [03:26<00:00, 51.6kB/s]\n",
      "\n",
      "Upload file tok2vec/model: 100%|██████████| 6.19M/6.19M [03:26<00:00, 31.4kB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "Upload file senter/model: 100%|██████████| 215k/215k [03:26<00:00, 1.06kB/s]\n",
      "\n",
      "\n",
      "\n",
      "Upload file vocab/key2row: 100%|██████████| 6.74M/6.74M [03:26<00:00, 34.2kB/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Upload file ner/model: 100%|██████████| 6.09M/6.09M [03:26<00:00, 30.9kB/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Upload file lemmatizer/lookups/lookups.bin: 100%|██████████| 950k/950k [03:26<00:00, 4.70kB/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Upload file parser/model: 100%|██████████| 312k/312k [03:26<00:00, 1.55kB/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Upload file tokenizer: 100%|██████████| 75.3k/75.3k [03:26<00:00, 373B/s]  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Upload file vocab/lookups.bin: 100%|██████████| 68.4k/68.4k [03:26<00:00, 339B/s]  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Upload file tagger/model: 100%|██████████| 19.4k/19.4k [03:26<00:00, 95.9B/s] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Upload file attribute_ruler/patterns: 100%|██████████| 14.4k/14.4k [03:26<00:00, 71.2B/s] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Upload file .DS_Store: 100%|██████████| 8.00k/8.00k [03:25<00:00, 39.8B/s] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Upload file parser/moves: 100%|██████████| 1.53k/1.53k [03:25<00:00, 7.63B/s] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Upload file ner/moves: 100%|██████████| 1.08k/1.08k [03:25<00:00, 5.38B/s] \n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "from huggingface_hub import Repository, login\n",
    "\n",
    "def save_model_to_huggingface(nlp, model_dir, repo_id, token):\n",
    "    \"\"\"\n",
    "    Save the trained spaCy model to Hugging Face Model Hub by cloning to a new folder and moving files.\n",
    "    \n",
    "    Args:\n",
    "        nlp (Language): The spaCy model to be saved.\n",
    "        model_dir (str): The directory path where the model will be saved.\n",
    "        repo_id (str): The repository ID on Hugging Face (e.g., 'Ranjithjames/SPACY_NER').\n",
    "        token (str): Your Hugging Face API token.\n",
    "    \"\"\"\n",
    "    # Save the spaCy model to the specified directory\n",
    "    nlp.to_disk(model_dir)\n",
    "\n",
    "    # Authenticate the Hugging Face Hub\n",
    "    login(token=token)\n",
    "    \n",
    "    # Temporary directory for cloning the repository\n",
    "    temp_model_dir = model_dir + \"_temp\"\n",
    "    \n",
    "    # Clone the repository to the temporary directory\n",
    "    repo_url = f\"https://huggingface.co/{repo_id}\"\n",
    "    repo = Repository(local_dir=temp_model_dir, clone_from=repo_url, use_auth_token=token)\n",
    "    \n",
    "    # Move the saved model files to the cloned repository directory\n",
    "    for item in os.listdir(model_dir):\n",
    "        s = os.path.join(model_dir, item)\n",
    "        d = os.path.join(temp_model_dir, item)\n",
    "        if os.path.isdir(s):\n",
    "            shutil.move(s, d)\n",
    "        else:\n",
    "            shutil.move(s, d)\n",
    "    \n",
    "    # Add and push the model files to the repository\n",
    "    repo.git_add(auto_lfs_track=True)\n",
    "    repo.git_commit(\"Update spaCy model\")\n",
    "    repo.git_push()\n",
    "    \n",
    "    # Clean up: move files back to original directory and remove temporary directory\n",
    "    for item in os.listdir(temp_model_dir):\n",
    "        s = os.path.join(temp_model_dir, item)\n",
    "        d = os.path.join(model_dir, item)\n",
    "        if os.path.isdir(s):\n",
    "            shutil.move(s, d)\n",
    "        else:\n",
    "            shutil.move(s, d)\n",
    "    shutil.rmtree(temp_model_dir)\n",
    "\n",
    "# Example usage\n",
    "model_dir = \"data/model\"\n",
    "repo_id = \"Ranjithjames/SPACY_NER\"\n",
    "token = \"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\"\n",
    "\n",
    "save_model_to_huggingface(newnlp, model_dir, repo_id, token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "def download_model_from_huggingface(repo_id, model_dir, token):\n",
    "    \"\"\"\n",
    "    Download the model from Hugging Face Model Hub and store it in the specified directory.\n",
    "    \n",
    "    Args:\n",
    "        repo_id (str): The repository ID on Hugging Face (e.g., 'Ranjithjames/SPACY_NER').\n",
    "        model_dir (str): The directory path where the model will be stored.\n",
    "        token (str): Your Hugging Face API token.\n",
    "    \"\"\"\n",
    "    # Create the model directory if it doesn't exist\n",
    "    if not os.path.exists(model_dir):\n",
    "        os.makedirs(model_dir)\n",
    "    \n",
    "    # Download the model files from the Hugging Face repository\n",
    "    files_to_download = [\"config.json\", \"tokenizer.json\", \"model.bin\", \"special_tokens_map.json\", \"vocab.txt\"]\n",
    "    \n",
    "    for file_name in files_to_download:\n",
    "        file_path = hf_hub_download(repo_id=repo_id, filename=file_name, use_auth_token=token)\n",
    "        shutil.move(file_path, os.path.join(model_dir, file_name))\n",
    "        \n",
    "# Example usage\n",
    "model_dir = \"data/model\"\n",
    "repo_id = \"Ranjithjames/SPACY_NER\"\n",
    "token = \"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\"\n",
    "\n",
    "download_model_from_huggingface(repo_id, model_dir, token)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 19/19 [00:03<00:00,  6.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9993\n",
      "Recall: 0.9994\n",
      "F1 Score: 0.9994\n",
      "Support: None\n",
      "Confusion Matrix:\n",
      "[[    0     2     0     0     0]\n",
      " [    0 18809     0     2     2]\n",
      " [    0     1   325     0     0]\n",
      " [    0     2     0    40     0]\n",
      " [    0     2     0     0    40]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     I-SKILL       0.00      0.00      0.00         2\n",
      "           O       1.00      1.00      1.00     18813\n",
      "     U-SKILL       1.00      1.00      1.00       326\n",
      "     B-SKILL       0.95      0.95      0.95        42\n",
      "     L-SKILL       0.95      0.95      0.95        42\n",
      "\n",
      "    accuracy                           1.00     19225\n",
      "   macro avg       0.78      0.78      0.78     19225\n",
      "weighted avg       1.00      1.00      1.00     19225\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/RanjithJames/.pyenv/versions/3.10.14/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/RanjithJames/.pyenv/versions/3.10.14/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/RanjithJames/.pyenv/versions/3.10.14/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/RanjithJames/.pyenv/versions/3.10.14/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix, classification_report\n",
    "from tqdm import tqdm\n",
    "from spacy.training import offsets_to_biluo_tags, biluo_tags_to_offsets\n",
    "import numpy as np\n",
    "\n",
    "# Load the model\n",
    "def load_model(model_dir):\n",
    "    \"\"\"\n",
    "    Load a spaCy model from a specified directory.\n",
    "    \n",
    "    Args:\n",
    "        model_dir (str): The directory path from where the model will be loaded.\n",
    "    \n",
    "    Returns:\n",
    "        Language: The loaded spaCy model.\n",
    "    \"\"\"\n",
    "    return spacy.load(model_dir)\n",
    "\n",
    "newnlp = load_model('data/model')\n",
    "\n",
    "def non_overlapping_entities(entities):\n",
    "    \"\"\"\n",
    "    Ensure entities are non-overlapping by sorting and filtering.\n",
    "    \n",
    "    Args:\n",
    "        entities (list of tuples): List of entities (start, end, label).\n",
    "    \n",
    "    Returns:\n",
    "        list of tuples: Filtered non-overlapping entities.\n",
    "    \"\"\"\n",
    "    sorted_entities = sorted(entities, key=lambda x: x[0])\n",
    "    non_overlapping = []\n",
    "\n",
    "    for ent in sorted_entities:\n",
    "        if not non_overlapping or ent[0] >= non_overlapping[-1][1]:\n",
    "            non_overlapping.append(ent)\n",
    "    return non_overlapping\n",
    "\n",
    "# Function to evaluate the model with progress tracking\n",
    "def evaluate_model(ner_model, test_data):\n",
    "    \"\"\"\n",
    "    Evaluate the NER model using the test data and calculate precision, recall, F1 scores, and support.\n",
    "    \n",
    "    Args:\n",
    "        ner_model (Language): The spaCy NER model.\n",
    "        test_data (list of tuples): A list where each tuple is of the form (text, {'entities': [(start, end, label), ...]})\n",
    "    \n",
    "    Returns:\n",
    "        dict: A dictionary containing precision, recall, F1 scores, support, and confusion matrix.\n",
    "    \"\"\"\n",
    "    true_entities = []\n",
    "    pred_entities = []\n",
    "\n",
    "    for text, annotations in tqdm(test_data, desc=\"Evaluating\"):\n",
    "        doc = ner_model(text)\n",
    "        true_ents = non_overlapping_entities(annotations['entities'])\n",
    "        pred_ents = [(ent.start_char, ent.end_char, ent.label_) for ent in doc.ents]\n",
    "        \n",
    "        # Generate token-based BIO tags for the true entities\n",
    "        true_tags = offsets_to_biluo_tags(ner_model.make_doc(text), true_ents)\n",
    "        \n",
    "        # Generate token-based BIO tags for the predicted entities\n",
    "        pred_tags = offsets_to_biluo_tags(ner_model.make_doc(text), pred_ents)\n",
    "        \n",
    "        # Append to the lists\n",
    "        true_entities.extend(true_tags)\n",
    "        pred_entities.extend(pred_tags)\n",
    "\n",
    "    # Calculate precision, recall, f1, and support\n",
    "    precision, recall, f1, support = precision_recall_fscore_support(true_entities, pred_entities, average='weighted')\n",
    "\n",
    "    # Generate confusion matrix\n",
    "    labels = list(set(true_entities + pred_entities))\n",
    "    conf_matrix = confusion_matrix(true_entities, pred_entities, labels=labels)\n",
    "    \n",
    "    # Generate classification report\n",
    "    class_report = classification_report(true_entities, pred_entities, labels=labels)\n",
    "\n",
    "    return {\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'support': support,\n",
    "        'confusion_matrix': conf_matrix,\n",
    "        'classification_report': class_report\n",
    "    }\n",
    "\n",
    "\n",
    "# Evaluate the model with progress tracking\n",
    "evaluation_results = evaluate_model(newnlp, val_data[1:20])\n",
    "\n",
    "print(\"Precision: {:.4f}\".format(evaluation_results['precision']))\n",
    "print(\"Recall: {:.4f}\".format(evaluation_results['recall']))\n",
    "print(\"F1 Score: {:.4f}\".format(evaluation_results['f1']))\n",
    "print(\"Support: {}\".format(evaluation_results['support']))\n",
    "print(\"Confusion Matrix:\\n{}\".format(evaluation_results['confusion_matrix']))\n",
    "print(\"Classification Report:\\n{}\".format(evaluation_results['classification_report']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
